{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "--- Generating Training/Validation Data (50000 samples) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating Test Data (500 samples) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Normalizing Data (using training split statistics) ---\n",
      "Return series mean: 0.000003, std: 0.019426\n",
      "Parameter means: ['0.0000', '0.0001', '0.0551', '0.0001', '0.8903', '0.2757', '0.0274']\n",
      "Parameter stds: ['0.0002', '0.0289', '0.0259', '0.0001', '0.0578', '0.1296', '0.0130']\n",
      "\n",
      "Train loader: 45000 samples. Val loader: 5000. Test loader: 500.\n",
      "Starting training with model: LSTMTransformerSVJD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.787473, Val Loss: 0.638409, Duration: 216.13s\n",
      "Test R2 Scores: R2(mju): 0.0480, R2(mjuJ): 0.7291, R2(sigmaJ): 0.3510, R2(varLT): 0.7684, R2(beta): 0.1921, R2(gamma): 0.0965, R2(lambda): 0.3424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Train Loss: 0.566306, Val Loss: 0.485251, Duration: 217.43s\n",
      "Test R2 Scores: R2(mju): 0.2620, R2(mjuJ): 0.7835, R2(sigmaJ): 0.6807, R2(varLT): 0.8922, R2(beta): 0.3372, R2(gamma): 0.2540, R2(lambda): 0.3950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Train Loss: 0.443098, Val Loss: 0.404642, Duration: 217.77s\n",
      "Test R2 Scores: R2(mju): 0.3675, R2(mjuJ): 0.8534, R2(sigmaJ): 0.7355, R2(varLT): 0.9183, R2(beta): 0.3530, R2(gamma): 0.4274, R2(lambda): 0.5129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Train Loss: 0.381013, Val Loss: 0.357107, Duration: 217.93s\n",
      "Test R2 Scores: R2(mju): 0.4015, R2(mjuJ): 0.8516, R2(sigmaJ): 0.8094, R2(varLT): 0.9274, R2(beta): 0.4036, R2(gamma): 0.5725, R2(lambda): 0.5733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Train Loss: 0.339749, Val Loss: 0.318470, Duration: 217.88s\n",
      "Test R2 Scores: R2(mju): 0.4069, R2(mjuJ): 0.8681, R2(sigmaJ): 0.8064, R2(varLT): 0.9317, R2(beta): 0.4630, R2(gamma): 0.6914, R2(lambda): 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Train Loss: 0.305590, Val Loss: 0.291254, Duration: 217.94s\n",
      "Test R2 Scores: R2(mju): 0.4225, R2(mjuJ): 0.8742, R2(sigmaJ): 0.8158, R2(varLT): 0.9344, R2(beta): 0.5295, R2(gamma): 0.7393, R2(lambda): 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Train Loss: 0.284505, Val Loss: 0.277727, Duration: 217.95s\n",
      "Test R2 Scores: R2(mju): 0.4549, R2(mjuJ): 0.8750, R2(sigmaJ): 0.8211, R2(varLT): 0.9387, R2(beta): 0.5452, R2(gamma): 0.7515, R2(lambda): 0.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Train Loss: 0.271890, Val Loss: 0.268686, Duration: 217.92s\n",
      "Test R2 Scores: R2(mju): 0.4658, R2(mjuJ): 0.8736, R2(sigmaJ): 0.8491, R2(varLT): 0.9369, R2(beta): 0.5456, R2(gamma): 0.7595, R2(lambda): 0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Train Loss: 0.262578, Val Loss: 0.255549, Duration: 217.96s\n",
      "Test R2 Scores: R2(mju): 0.4432, R2(mjuJ): 0.8792, R2(sigmaJ): 0.8475, R2(varLT): 0.9382, R2(beta): 0.6059, R2(gamma): 0.7814, R2(lambda): 0.7050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Train Loss: 0.255294, Val Loss: 0.268612, Duration: 217.95s\n",
      "Test R2 Scores: R2(mju): 0.4627, R2(mjuJ): 0.8784, R2(sigmaJ): 0.8296, R2(varLT): 0.9407, R2(beta): 0.5745, R2(gamma): 0.7373, R2(lambda): 0.6946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Train Loss: 0.249711, Val Loss: 0.248871, Duration: 217.61s\n",
      "Test R2 Scores: R2(mju): 0.4737, R2(mjuJ): 0.8789, R2(sigmaJ): 0.8491, R2(varLT): 0.9430, R2(beta): 0.6270, R2(gamma): 0.7912, R2(lambda): 0.7108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Train Loss: 0.244803, Val Loss: 0.247634, Duration: 217.49s\n",
      "Test R2 Scores: R2(mju): 0.4751, R2(mjuJ): 0.8766, R2(sigmaJ): 0.8403, R2(varLT): 0.9418, R2(beta): 0.6250, R2(gamma): 0.7666, R2(lambda): 0.7231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 - Train Loss: 0.240155, Val Loss: 0.235753, Duration: 217.53s\n",
      "Test R2 Scores: R2(mju): 0.4748, R2(mjuJ): 0.8805, R2(sigmaJ): 0.8669, R2(varLT): 0.9443, R2(beta): 0.6461, R2(gamma): 0.8171, R2(lambda): 0.7238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 - Train Loss: 0.236358, Val Loss: 0.238833, Duration: 217.63s\n",
      "Test R2 Scores: R2(mju): 0.4802, R2(mjuJ): 0.8829, R2(sigmaJ): 0.8638, R2(varLT): 0.9419, R2(beta): 0.6415, R2(gamma): 0.7878, R2(lambda): 0.7277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 - Train Loss: 0.233514, Val Loss: 0.235309, Duration: 217.52s\n",
      "Test R2 Scores: R2(mju): 0.4809, R2(mjuJ): 0.8826, R2(sigmaJ): 0.8773, R2(varLT): 0.9420, R2(beta): 0.6535, R2(gamma): 0.7756, R2(lambda): 0.7322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Train Loss: 0.230277, Val Loss: 0.236615, Duration: 217.50s\n",
      "Test R2 Scores: R2(mju): 0.4412, R2(mjuJ): 0.8846, R2(sigmaJ): 0.8750, R2(varLT): 0.9447, R2(beta): 0.6611, R2(gamma): 0.8273, R2(lambda): 0.7283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 - Train Loss: 0.227794, Val Loss: 0.229389, Duration: 217.54s\n",
      "Test R2 Scores: R2(mju): 0.4889, R2(mjuJ): 0.8838, R2(sigmaJ): 0.8811, R2(varLT): 0.9448, R2(beta): 0.6380, R2(gamma): 0.8100, R2(lambda): 0.7326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Train Loss: 0.225815, Val Loss: 0.240326, Duration: 217.65s\n",
      "Test R2 Scores: R2(mju): 0.4589, R2(mjuJ): 0.8848, R2(sigmaJ): 0.8843, R2(varLT): 0.9463, R2(beta): 0.6301, R2(gamma): 0.7619, R2(lambda): 0.7364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 - Train Loss: 0.224026, Val Loss: 0.227795, Duration: 217.53s\n",
      "Test R2 Scores: R2(mju): 0.4852, R2(mjuJ): 0.8834, R2(sigmaJ): 0.8619, R2(varLT): 0.9469, R2(beta): 0.6725, R2(gamma): 0.8222, R2(lambda): 0.7390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 - Train Loss: 0.222381, Val Loss: 0.228297, Duration: 217.56s\n",
      "Test R2 Scores: R2(mju): 0.4753, R2(mjuJ): 0.8877, R2(sigmaJ): 0.8635, R2(varLT): 0.9462, R2(beta): 0.6827, R2(gamma): 0.8349, R2(lambda): 0.7073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 - Train Loss: 0.220582, Val Loss: 0.232478, Duration: 217.48s\n",
      "Test R2 Scores: R2(mju): 0.4461, R2(mjuJ): 0.8753, R2(sigmaJ): 0.8680, R2(varLT): 0.9420, R2(beta): 0.6825, R2(gamma): 0.8394, R2(lambda): 0.7395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 - Train Loss: 0.218816, Val Loss: 0.221455, Duration: 217.57s\n",
      "Test R2 Scores: R2(mju): 0.4901, R2(mjuJ): 0.8871, R2(sigmaJ): 0.8859, R2(varLT): 0.9473, R2(beta): 0.6775, R2(gamma): 0.8318, R2(lambda): 0.7362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 - Train Loss: 0.218261, Val Loss: 0.219212, Duration: 217.46s\n",
      "Test R2 Scores: R2(mju): 0.4933, R2(mjuJ): 0.8855, R2(sigmaJ): 0.8835, R2(varLT): 0.9468, R2(beta): 0.6758, R2(gamma): 0.8302, R2(lambda): 0.7395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 - Train Loss: 0.216501, Val Loss: 0.221114, Duration: 217.46s\n",
      "Test R2 Scores: R2(mju): 0.4961, R2(mjuJ): 0.8872, R2(sigmaJ): 0.8944, R2(varLT): 0.9439, R2(beta): 0.6739, R2(gamma): 0.8339, R2(lambda): 0.7209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 - Train Loss: 0.215070, Val Loss: 0.222997, Duration: 217.44s\n",
      "Test R2 Scores: R2(mju): 0.4997, R2(mjuJ): 0.8865, R2(sigmaJ): 0.8896, R2(varLT): 0.9486, R2(beta): 0.6612, R2(gamma): 0.8228, R2(lambda): 0.7513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 - Train Loss: 0.214197, Val Loss: 0.225920, Duration: 217.21s\n",
      "Test R2 Scores: R2(mju): 0.4797, R2(mjuJ): 0.8846, R2(sigmaJ): 0.8616, R2(varLT): 0.9497, R2(beta): 0.6754, R2(gamma): 0.8375, R2(lambda): 0.7267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 - Train Loss: 0.213574, Val Loss: 0.233003, Duration: 217.24s\n",
      "Test R2 Scores: R2(mju): 0.4740, R2(mjuJ): 0.8872, R2(sigmaJ): 0.8847, R2(varLT): 0.9500, R2(beta): 0.6395, R2(gamma): 0.8176, R2(lambda): 0.7258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 - Train Loss: 0.212752, Val Loss: 0.219232, Duration: 217.14s\n",
      "Test R2 Scores: R2(mju): 0.4892, R2(mjuJ): 0.8894, R2(sigmaJ): 0.8841, R2(varLT): 0.9481, R2(beta): 0.6888, R2(gamma): 0.8456, R2(lambda): 0.7367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 - Train Loss: 0.211855, Val Loss: 0.220369, Duration: 217.12s\n",
      "Test R2 Scores: R2(mju): 0.4884, R2(mjuJ): 0.8906, R2(sigmaJ): 0.8889, R2(varLT): 0.9487, R2(beta): 0.6806, R2(gamma): 0.8279, R2(lambda): 0.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 - Train Loss: 0.210760, Val Loss: 0.218897, Duration: 217.23s\n",
      "Test R2 Scores: R2(mju): 0.5002, R2(mjuJ): 0.8895, R2(sigmaJ): 0.8713, R2(varLT): 0.9496, R2(beta): 0.6953, R2(gamma): 0.8375, R2(lambda): 0.7296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 - Train Loss: 0.209798, Val Loss: 0.226395, Duration: 217.22s\n",
      "Test R2 Scores: R2(mju): 0.4909, R2(mjuJ): 0.8815, R2(sigmaJ): 0.8580, R2(varLT): 0.9512, R2(beta): 0.6753, R2(gamma): 0.8382, R2(lambda): 0.7215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 - Train Loss: 0.209399, Val Loss: 0.217259, Duration: 217.14s\n",
      "Test R2 Scores: R2(mju): 0.4747, R2(mjuJ): 0.8915, R2(sigmaJ): 0.8981, R2(varLT): 0.9488, R2(beta): 0.6823, R2(gamma): 0.8154, R2(lambda): 0.7311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 - Train Loss: 0.208808, Val Loss: 0.223160, Duration: 217.18s\n",
      "Test R2 Scores: R2(mju): 0.4901, R2(mjuJ): 0.8889, R2(sigmaJ): 0.8942, R2(varLT): 0.9511, R2(beta): 0.6399, R2(gamma): 0.8193, R2(lambda): 0.7407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 - Train Loss: 0.207860, Val Loss: 0.220539, Duration: 217.26s\n",
      "Test R2 Scores: R2(mju): 0.4958, R2(mjuJ): 0.8903, R2(sigmaJ): 0.9010, R2(varLT): 0.9502, R2(beta): 0.6714, R2(gamma): 0.8351, R2(lambda): 0.7259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 - Train Loss: 0.207531, Val Loss: 0.215305, Duration: 217.20s\n",
      "Test R2 Scores: R2(mju): 0.4711, R2(mjuJ): 0.8920, R2(sigmaJ): 0.8991, R2(varLT): 0.9489, R2(beta): 0.6929, R2(gamma): 0.8510, R2(lambda): 0.7467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 - Train Loss: 0.206623, Val Loss: 0.220081, Duration: 217.18s\n",
      "Test R2 Scores: R2(mju): 0.4977, R2(mjuJ): 0.8895, R2(sigmaJ): 0.9001, R2(varLT): 0.9498, R2(beta): 0.6686, R2(gamma): 0.8130, R2(lambda): 0.7419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 - Train Loss: 0.205923, Val Loss: 0.213076, Duration: 217.18s\n",
      "Test R2 Scores: R2(mju): 0.4882, R2(mjuJ): 0.8916, R2(sigmaJ): 0.8976, R2(varLT): 0.9518, R2(beta): 0.7002, R2(gamma): 0.8507, R2(lambda): 0.7333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 - Train Loss: 0.205733, Val Loss: 0.216642, Duration: 217.17s\n",
      "Test R2 Scores: R2(mju): 0.4898, R2(mjuJ): 0.8906, R2(sigmaJ): 0.8812, R2(varLT): 0.9514, R2(beta): 0.6913, R2(gamma): 0.8305, R2(lambda): 0.7399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 - Train Loss: 0.205079, Val Loss: 0.216442, Duration: 217.23s\n",
      "Test R2 Scores: R2(mju): 0.4927, R2(mjuJ): 0.8924, R2(sigmaJ): 0.8964, R2(varLT): 0.9517, R2(beta): 0.6824, R2(gamma): 0.8090, R2(lambda): 0.7473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 - Train Loss: 0.204616, Val Loss: 0.220132, Duration: 217.27s\n",
      "Test R2 Scores: R2(mju): 0.4981, R2(mjuJ): 0.8925, R2(sigmaJ): 0.8933, R2(varLT): 0.9472, R2(beta): 0.6478, R2(gamma): 0.8409, R2(lambda): 0.7462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 - Train Loss: 0.203792, Val Loss: 0.212993, Duration: 217.21s\n",
      "Test R2 Scores: R2(mju): 0.4922, R2(mjuJ): 0.8874, R2(sigmaJ): 0.9024, R2(varLT): 0.9513, R2(beta): 0.6977, R2(gamma): 0.8458, R2(lambda): 0.7538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 - Train Loss: 0.203827, Val Loss: 0.214483, Duration: 217.23s\n",
      "Test R2 Scores: R2(mju): 0.4891, R2(mjuJ): 0.8918, R2(sigmaJ): 0.8956, R2(varLT): 0.9506, R2(beta): 0.6871, R2(gamma): 0.8399, R2(lambda): 0.7368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 - Train Loss: 0.203106, Val Loss: 0.218951, Duration: 217.25s\n",
      "Test R2 Scores: R2(mju): 0.4959, R2(mjuJ): 0.8899, R2(sigmaJ): 0.9003, R2(varLT): 0.9501, R2(beta): 0.6562, R2(gamma): 0.8275, R2(lambda): 0.7331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 - Train Loss: 0.202146, Val Loss: 0.218036, Duration: 217.22s\n",
      "Test R2 Scores: R2(mju): 0.5034, R2(mjuJ): 0.8895, R2(sigmaJ): 0.8963, R2(varLT): 0.9501, R2(beta): 0.6706, R2(gamma): 0.8398, R2(lambda): 0.7281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 - Train Loss: 0.202227, Val Loss: 0.214451, Duration: 217.30s\n",
      "Test R2 Scores: R2(mju): 0.4843, R2(mjuJ): 0.8921, R2(sigmaJ): 0.8963, R2(varLT): 0.9516, R2(beta): 0.7050, R2(gamma): 0.8383, R2(lambda): 0.7408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 - Train Loss: 0.201992, Val Loss: 0.217722, Duration: 217.38s\n",
      "Test R2 Scores: R2(mju): 0.4999, R2(mjuJ): 0.8944, R2(sigmaJ): 0.9031, R2(varLT): 0.9512, R2(beta): 0.6566, R2(gamma): 0.8397, R2(lambda): 0.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 - Train Loss: 0.201379, Val Loss: 0.213818, Duration: 217.39s\n",
      "Test R2 Scores: R2(mju): 0.4849, R2(mjuJ): 0.8900, R2(sigmaJ): 0.8963, R2(varLT): 0.9498, R2(beta): 0.7042, R2(gamma): 0.8463, R2(lambda): 0.7502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 - Train Loss: 0.200860, Val Loss: 0.213401, Duration: 217.42s\n",
      "Test R2 Scores: R2(mju): 0.4897, R2(mjuJ): 0.8957, R2(sigmaJ): 0.9016, R2(varLT): 0.9507, R2(beta): 0.6990, R2(gamma): 0.8393, R2(lambda): 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 - Train Loss: 0.200393, Val Loss: 0.212906, Duration: 217.49s\n",
      "Test R2 Scores: R2(mju): 0.4968, R2(mjuJ): 0.8950, R2(sigmaJ): 0.8893, R2(varLT): 0.9524, R2(beta): 0.6952, R2(gamma): 0.8503, R2(lambda): 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 - Train Loss: 0.200076, Val Loss: 0.211110, Duration: 217.45s\n",
      "Test R2 Scores: R2(mju): 0.4650, R2(mjuJ): 0.8914, R2(sigmaJ): 0.8971, R2(varLT): 0.9493, R2(beta): 0.7069, R2(gamma): 0.8538, R2(lambda): 0.7423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 - Train Loss: 0.199765, Val Loss: 0.221273, Duration: 217.48s\n",
      "Test R2 Scores: R2(mju): 0.4615, R2(mjuJ): 0.8922, R2(sigmaJ): 0.8797, R2(varLT): 0.9512, R2(beta): 0.6951, R2(gamma): 0.8243, R2(lambda): 0.7349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 - Train Loss: 0.199298, Val Loss: 0.214811, Duration: 217.52s\n",
      "Test R2 Scores: R2(mju): 0.4680, R2(mjuJ): 0.8921, R2(sigmaJ): 0.8960, R2(varLT): 0.9520, R2(beta): 0.7036, R2(gamma): 0.8343, R2(lambda): 0.7463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 - Train Loss: 0.199017, Val Loss: 0.215783, Duration: 217.44s\n",
      "Test R2 Scores: R2(mju): 0.4893, R2(mjuJ): 0.8970, R2(sigmaJ): 0.8949, R2(varLT): 0.9505, R2(beta): 0.6900, R2(gamma): 0.8463, R2(lambda): 0.7296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 - Train Loss: 0.198575, Val Loss: 0.218561, Duration: 217.47s\n",
      "Test R2 Scores: R2(mju): 0.5048, R2(mjuJ): 0.8937, R2(sigmaJ): 0.8979, R2(varLT): 0.9517, R2(beta): 0.6847, R2(gamma): 0.8026, R2(lambda): 0.7310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 - Train Loss: 0.198637, Val Loss: 0.217685, Duration: 217.39s\n",
      "Test R2 Scores: R2(mju): 0.4980, R2(mjuJ): 0.8961, R2(sigmaJ): 0.8920, R2(varLT): 0.9514, R2(beta): 0.6659, R2(gamma): 0.8471, R2(lambda): 0.7398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 - Train Loss: 0.198100, Val Loss: 0.217727, Duration: 217.45s\n",
      "Test R2 Scores: R2(mju): 0.5021, R2(mjuJ): 0.8930, R2(sigmaJ): 0.8912, R2(varLT): 0.9508, R2(beta): 0.6936, R2(gamma): 0.8355, R2(lambda): 0.7243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 - Train Loss: 0.197423, Val Loss: 0.215819, Duration: 217.45s\n",
      "Test R2 Scores: R2(mju): 0.4941, R2(mjuJ): 0.8902, R2(sigmaJ): 0.8930, R2(varLT): 0.9504, R2(beta): 0.6849, R2(gamma): 0.8351, R2(lambda): 0.7460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 - Train Loss: 0.197725, Val Loss: 0.214341, Duration: 217.46s\n",
      "Test R2 Scores: R2(mju): 0.4966, R2(mjuJ): 0.8937, R2(sigmaJ): 0.8992, R2(varLT): 0.9515, R2(beta): 0.6784, R2(gamma): 0.8441, R2(lambda): 0.7297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 - Train Loss: 0.196819, Val Loss: 0.209484, Duration: 217.39s\n",
      "Test R2 Scores: R2(mju): 0.4936, R2(mjuJ): 0.8955, R2(sigmaJ): 0.8980, R2(varLT): 0.9514, R2(beta): 0.7087, R2(gamma): 0.8580, R2(lambda): 0.7319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 - Train Loss: 0.196680, Val Loss: 0.210604, Duration: 217.42s\n",
      "Test R2 Scores: R2(mju): 0.4983, R2(mjuJ): 0.8961, R2(sigmaJ): 0.9015, R2(varLT): 0.9508, R2(beta): 0.6940, R2(gamma): 0.8429, R2(lambda): 0.7442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 - Train Loss: 0.196439, Val Loss: 0.208888, Duration: 217.44s\n",
      "Test R2 Scores: R2(mju): 0.4957, R2(mjuJ): 0.8925, R2(sigmaJ): 0.9058, R2(varLT): 0.9520, R2(beta): 0.7217, R2(gamma): 0.8572, R2(lambda): 0.7420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 - Train Loss: 0.195818, Val Loss: 0.210170, Duration: 217.44s\n",
      "Test R2 Scores: R2(mju): 0.4948, R2(mjuJ): 0.8981, R2(sigmaJ): 0.8996, R2(varLT): 0.9514, R2(beta): 0.6977, R2(gamma): 0.8531, R2(lambda): 0.7497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 - Train Loss: 0.195841, Val Loss: 0.224753, Duration: 217.44s\n",
      "Test R2 Scores: R2(mju): 0.4621, R2(mjuJ): 0.8922, R2(sigmaJ): 0.8822, R2(varLT): 0.9477, R2(beta): 0.6819, R2(gamma): 0.8181, R2(lambda): 0.7151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 - Train Loss: 0.195446, Val Loss: 0.213099, Duration: 217.43s\n",
      "Test R2 Scores: R2(mju): 0.4812, R2(mjuJ): 0.8933, R2(sigmaJ): 0.8878, R2(varLT): 0.9515, R2(beta): 0.7029, R2(gamma): 0.8412, R2(lambda): 0.7298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 - Train Loss: 0.195317, Val Loss: 0.211409, Duration: 217.46s\n",
      "Test R2 Scores: R2(mju): 0.5022, R2(mjuJ): 0.8948, R2(sigmaJ): 0.9019, R2(varLT): 0.9526, R2(beta): 0.6850, R2(gamma): 0.8482, R2(lambda): 0.7349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 - Train Loss: 0.194891, Val Loss: 0.212722, Duration: 217.49s\n",
      "Test R2 Scores: R2(mju): 0.4901, R2(mjuJ): 0.8935, R2(sigmaJ): 0.9046, R2(varLT): 0.9514, R2(beta): 0.7078, R2(gamma): 0.8232, R2(lambda): 0.7489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 - Train Loss: 0.194769, Val Loss: 0.209196, Duration: 217.48s\n",
      "Test R2 Scores: R2(mju): 0.4898, R2(mjuJ): 0.8962, R2(sigmaJ): 0.9020, R2(varLT): 0.9509, R2(beta): 0.7109, R2(gamma): 0.8501, R2(lambda): 0.7389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 - Train Loss: 0.194065, Val Loss: 0.211389, Duration: 217.45s\n",
      "Test R2 Scores: R2(mju): 0.4974, R2(mjuJ): 0.8952, R2(sigmaJ): 0.8860, R2(varLT): 0.9522, R2(beta): 0.7010, R2(gamma): 0.8462, R2(lambda): 0.7289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 - Train Loss: 0.193756, Val Loss: 0.212939, Duration: 217.53s\n",
      "Test R2 Scores: R2(mju): 0.4958, R2(mjuJ): 0.8911, R2(sigmaJ): 0.8872, R2(varLT): 0.9522, R2(beta): 0.7058, R2(gamma): 0.8429, R2(lambda): 0.7319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 - Train Loss: 0.193744, Val Loss: 0.218790, Duration: 217.51s\n",
      "Test R2 Scores: R2(mju): 0.4760, R2(mjuJ): 0.8977, R2(sigmaJ): 0.9053, R2(varLT): 0.9521, R2(beta): 0.6555, R2(gamma): 0.8391, R2(lambda): 0.7440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 - Train Loss: 0.193358, Val Loss: 0.215895, Duration: 217.55s\n",
      "Test R2 Scores: R2(mju): 0.4626, R2(mjuJ): 0.8879, R2(sigmaJ): 0.8982, R2(varLT): 0.9511, R2(beta): 0.7034, R2(gamma): 0.8495, R2(lambda): 0.7493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100 - Train Loss: 0.193083, Val Loss: 0.217380, Duration: 217.55s\n",
      "Test R2 Scores: R2(mju): 0.4945, R2(mjuJ): 0.8933, R2(sigmaJ): 0.8790, R2(varLT): 0.9505, R2(beta): 0.7008, R2(gamma): 0.8293, R2(lambda): 0.7257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 - Train Loss: 0.192724, Val Loss: 0.209072, Duration: 217.52s\n",
      "Test R2 Scores: R2(mju): 0.4986, R2(mjuJ): 0.8945, R2(sigmaJ): 0.9015, R2(varLT): 0.9525, R2(beta): 0.6869, R2(gamma): 0.8461, R2(lambda): 0.7496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 - Train Loss: 0.192499, Val Loss: 0.216918, Duration: 217.56s\n",
      "Test R2 Scores: R2(mju): 0.4708, R2(mjuJ): 0.8890, R2(sigmaJ): 0.9014, R2(varLT): 0.9500, R2(beta): 0.6923, R2(gamma): 0.8482, R2(lambda): 0.7254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 - Train Loss: 0.192246, Val Loss: 0.209844, Duration: 217.51s\n",
      "Test R2 Scores: R2(mju): 0.4969, R2(mjuJ): 0.8961, R2(sigmaJ): 0.9011, R2(varLT): 0.9507, R2(beta): 0.7043, R2(gamma): 0.8519, R2(lambda): 0.7425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 - Train Loss: 0.191910, Val Loss: 0.215762, Duration: 217.58s\n",
      "Test R2 Scores: R2(mju): 0.4941, R2(mjuJ): 0.8950, R2(sigmaJ): 0.9032, R2(varLT): 0.9494, R2(beta): 0.6737, R2(gamma): 0.8503, R2(lambda): 0.7385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 - Train Loss: 0.191633, Val Loss: 0.209539, Duration: 217.50s\n",
      "Test R2 Scores: R2(mju): 0.4948, R2(mjuJ): 0.8924, R2(sigmaJ): 0.9066, R2(varLT): 0.9517, R2(beta): 0.7074, R2(gamma): 0.8447, R2(lambda): 0.7302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 - Train Loss: 0.191025, Val Loss: 0.213002, Duration: 217.57s\n",
      "Test R2 Scores: R2(mju): 0.4866, R2(mjuJ): 0.8951, R2(sigmaJ): 0.9018, R2(varLT): 0.9525, R2(beta): 0.6855, R2(gamma): 0.8472, R2(lambda): 0.7354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 - Train Loss: 0.190999, Val Loss: 0.223397, Duration: 217.52s\n",
      "Test R2 Scores: R2(mju): 0.4700, R2(mjuJ): 0.8942, R2(sigmaJ): 0.8888, R2(varLT): 0.9513, R2(beta): 0.6963, R2(gamma): 0.8245, R2(lambda): 0.7107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 - Train Loss: 0.190796, Val Loss: 0.208801, Duration: 217.54s\n",
      "Test R2 Scores: R2(mju): 0.4912, R2(mjuJ): 0.8926, R2(sigmaJ): 0.9036, R2(varLT): 0.9503, R2(beta): 0.7046, R2(gamma): 0.8530, R2(lambda): 0.7341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 - Train Loss: 0.190913, Val Loss: 0.211418, Duration: 217.59s\n",
      "Test R2 Scores: R2(mju): 0.4926, R2(mjuJ): 0.8960, R2(sigmaJ): 0.8883, R2(varLT): 0.9521, R2(beta): 0.6923, R2(gamma): 0.8487, R2(lambda): 0.7441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100 - Train Loss: 0.190378, Val Loss: 0.215430, Duration: 217.63s\n",
      "Test R2 Scores: R2(mju): 0.4556, R2(mjuJ): 0.8945, R2(sigmaJ): 0.9009, R2(varLT): 0.9522, R2(beta): 0.6858, R2(gamma): 0.8344, R2(lambda): 0.7291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100 - Train Loss: 0.189489, Val Loss: 0.211467, Duration: 217.66s\n",
      "Test R2 Scores: R2(mju): 0.4892, R2(mjuJ): 0.8964, R2(sigmaJ): 0.8977, R2(varLT): 0.9519, R2(beta): 0.7032, R2(gamma): 0.8443, R2(lambda): 0.7178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100 - Train Loss: 0.189934, Val Loss: 0.210511, Duration: 217.61s\n",
      "Test R2 Scores: R2(mju): 0.4976, R2(mjuJ): 0.8912, R2(sigmaJ): 0.9044, R2(varLT): 0.9519, R2(beta): 0.6993, R2(gamma): 0.8507, R2(lambda): 0.7321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100 - Train Loss: 0.189499, Val Loss: 0.214223, Duration: 217.63s\n",
      "Test R2 Scores: R2(mju): 0.4924, R2(mjuJ): 0.8936, R2(sigmaJ): 0.9027, R2(varLT): 0.9508, R2(beta): 0.6912, R2(gamma): 0.8382, R2(lambda): 0.7322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100 - Train Loss: 0.188828, Val Loss: 0.212750, Duration: 217.65s\n",
      "Test R2 Scores: R2(mju): 0.4953, R2(mjuJ): 0.8956, R2(sigmaJ): 0.9006, R2(varLT): 0.9501, R2(beta): 0.6999, R2(gamma): 0.8233, R2(lambda): 0.7447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100 - Train Loss: 0.188680, Val Loss: 0.220784, Duration: 217.60s\n",
      "Test R2 Scores: R2(mju): 0.4770, R2(mjuJ): 0.8916, R2(sigmaJ): 0.8872, R2(varLT): 0.9529, R2(beta): 0.6756, R2(gamma): 0.8470, R2(lambda): 0.7065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100 - Train Loss: 0.188175, Val Loss: 0.210583, Duration: 217.56s\n",
      "Test R2 Scores: R2(mju): 0.4902, R2(mjuJ): 0.8937, R2(sigmaJ): 0.9010, R2(varLT): 0.9514, R2(beta): 0.7119, R2(gamma): 0.8530, R2(lambda): 0.7339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100 - Train Loss: 0.188138, Val Loss: 0.215291, Duration: 217.62s\n",
      "Test R2 Scores: R2(mju): 0.4930, R2(mjuJ): 0.8937, R2(sigmaJ): 0.9000, R2(varLT): 0.9530, R2(beta): 0.6732, R2(gamma): 0.8497, R2(lambda): 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100 - Train Loss: 0.187324, Val Loss: 0.212166, Duration: 217.64s\n",
      "Test R2 Scores: R2(mju): 0.4810, R2(mjuJ): 0.8937, R2(sigmaJ): 0.9027, R2(varLT): 0.9522, R2(beta): 0.6792, R2(gamma): 0.8416, R2(lambda): 0.7305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100 - Train Loss: 0.187527, Val Loss: 0.211372, Duration: 217.62s\n",
      "Test R2 Scores: R2(mju): 0.4854, R2(mjuJ): 0.8956, R2(sigmaJ): 0.8942, R2(varLT): 0.9528, R2(beta): 0.7005, R2(gamma): 0.8389, R2(lambda): 0.7378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100 - Train Loss: 0.187096, Val Loss: 0.214254, Duration: 217.59s\n",
      "Test R2 Scores: R2(mju): 0.4783, R2(mjuJ): 0.8913, R2(sigmaJ): 0.8943, R2(varLT): 0.9518, R2(beta): 0.6896, R2(gamma): 0.8485, R2(lambda): 0.7194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100 - Train Loss: 0.186883, Val Loss: 0.214739, Duration: 217.61s\n",
      "Test R2 Scores: R2(mju): 0.4900, R2(mjuJ): 0.8951, R2(sigmaJ): 0.8952, R2(varLT): 0.9503, R2(beta): 0.6917, R2(gamma): 0.8301, R2(lambda): 0.7399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100 - Train Loss: 0.186663, Val Loss: 0.221414, Duration: 217.72s\n",
      "Test R2 Scores: R2(mju): 0.4966, R2(mjuJ): 0.8952, R2(sigmaJ): 0.8919, R2(varLT): 0.9508, R2(beta): 0.6711, R2(gamma): 0.8464, R2(lambda): 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100 - Train Loss: 0.186378, Val Loss: 0.213942, Duration: 217.63s\n",
      "Test R2 Scores: R2(mju): 0.4852, R2(mjuJ): 0.8953, R2(sigmaJ): 0.8775, R2(varLT): 0.9525, R2(beta): 0.6967, R2(gamma): 0.8523, R2(lambda): 0.7249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100 - Train Loss: 0.186083, Val Loss: 0.213816, Duration: 217.57s\n",
      "Test R2 Scores: R2(mju): 0.4817, R2(mjuJ): 0.8937, R2(sigmaJ): 0.8936, R2(varLT): 0.9505, R2(beta): 0.6975, R2(gamma): 0.8501, R2(lambda): 0.7096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100 - Train Loss: 0.185770, Val Loss: 0.217068, Duration: 217.66s\n",
      "Test R2 Scores: R2(mju): 0.4440, R2(mjuJ): 0.8960, R2(sigmaJ): 0.8882, R2(varLT): 0.9505, R2(beta): 0.6915, R2(gamma): 0.8446, R2(lambda): 0.7372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100 - Train Loss: 0.185594, Val Loss: 0.219869, Duration: 217.64s\n",
      "Test R2 Scores: R2(mju): 0.4380, R2(mjuJ): 0.8923, R2(sigmaJ): 0.8973, R2(varLT): 0.9519, R2(beta): 0.6709, R2(gamma): 0.8274, R2(lambda): 0.7175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100 - Train Loss: 0.185167, Val Loss: 0.214734, Duration: 217.73s\n",
      "Test R2 Scores: R2(mju): 0.4777, R2(mjuJ): 0.8931, R2(sigmaJ): 0.8888, R2(varLT): 0.9533, R2(beta): 0.6901, R2(gamma): 0.8496, R2(lambda): 0.7286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_591516/3443863026.py:328: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 - Train Loss: 0.184581, Val Loss: 0.216312, Duration: 217.66s\n",
      "Test R2 Scores: R2(mju): 0.4794, R2(mjuJ): 0.8904, R2(sigmaJ): 0.9006, R2(varLT): 0.9504, R2(beta): 0.6874, R2(gamma): 0.8473, R2(lambda): 0.7343\n",
      "Early stopping triggered after 20 epochs without improvement on validation set.\n",
      "Training finished.\n",
      "Loading best model weights from best_svjd_lstmtransformersvjd_model.pth based on validation loss...\n",
      "\n",
      "--- Final R2 Scores on Test Set (LSTMTransformerSVJD) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2(mju): 0.4912, R2(mjuJ): 0.8926, R2(sigmaJ): 0.9036, R2(varLT): 0.9503, R2(beta): 0.7046, R2(gamma): 0.8530, R2(lambda): 0.7341\n",
      "\n",
      "--- Evaluating model LSTMTransformerSVJD on 100 samples with fixed parameters ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting on fixed-param samples: 100%|██████████| 7/7 [00:00<00:00, 51.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation with fixed parameters complete. ---\n",
      "\n",
      "DataFrame from fixed parameter evaluation (LSTMTransformerSVJD, first 5 rows):\n",
      "   mju_true  mju_pred  mjuJ_true  mjuJ_pred  sigmaJ_true  sigmaJ_pred  \\\n",
      "0    0.0001  0.000106       0.01   0.020303         0.05     0.058226   \n",
      "1    0.0001 -0.000018       0.01   0.022349         0.05     0.044525   \n",
      "2    0.0001  0.000028       0.01   0.024988         0.05     0.057359   \n",
      "3    0.0001 -0.000080       0.01   0.000132         0.05     0.048335   \n",
      "4    0.0001 -0.000047       0.01   0.009099         0.05     0.068382   \n",
      "\n",
      "   varLT_true  varLT_pred  beta_true  beta_pred  gamma_true  gamma_pred  \\\n",
      "0      0.0002    0.000208       0.95   0.933085         0.2    0.229987   \n",
      "1      0.0002    0.000177       0.95   0.938470         0.2    0.195718   \n",
      "2      0.0002    0.000194       0.95   0.955397         0.2    0.222911   \n",
      "3      0.0002    0.000193       0.95   0.900386         0.2    0.305399   \n",
      "4      0.0002    0.000179       0.95   0.959047         0.2    0.165144   \n",
      "\n",
      "   lambda_true  lambda_pred  \n",
      "0         0.02     0.020700  \n",
      "1         0.02     0.025369  \n",
      "2         0.02     0.019290  \n",
      "3         0.02     0.025772  \n",
      "4         0.02     0.013361  \n",
      "\n",
      "--- Testing LSTMTransformerSVJD for 'gamma' from 0.0500 to 0.5000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing range for gamma: 100%|██████████| 10/10 [00:00<00:00, 110.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Parameter range testing for 'gamma' complete. ---\n",
      "\n",
      "DataFrame from 'gamma' range test (LSTMTransformerSVJD, first 5 rows):\n",
      "   gamma_true  mju_pred  mjuJ_pred  sigmaJ_pred  varLT_pred  beta_pred  \\\n",
      "0        0.05  0.000071   0.013362     0.059445    0.000206   0.840341   \n",
      "1        0.10  0.000255   0.017185     0.046173    0.000187   0.936881   \n",
      "2        0.15  0.000053   0.016413     0.051196    0.000179   0.926072   \n",
      "3        0.20  0.000120   0.020639     0.056835    0.000207   0.873445   \n",
      "4        0.25 -0.000129   0.020157     0.068087    0.000168   0.952358   \n",
      "\n",
      "   gamma_pred  lambda_pred  \n",
      "0    0.082759     0.017765  \n",
      "1    0.115821     0.025518  \n",
      "2    0.188630     0.024458  \n",
      "3    0.310046     0.019021  \n",
      "4    0.246619     0.014540  \n",
      "\n",
      "Script finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import math # For Positional Encoding\n",
    "\n",
    "# --- Configuration ---\n",
    "# Data Generation Parameters\n",
    "N_SAMPLES_TOTAL = 50000\n",
    "N_TEST_SAMPLES = 500\n",
    "T_LENGTH = 2000\n",
    "VAL_SPLIT_RATIO = 0.1\n",
    "\n",
    "# SVJD Model Parameter Ranges\n",
    "MU_RANGE = (-0.1 / 250, 0.1 / 250)\n",
    "V_LT_RANGE = (0.005**2, 0.015**2) \n",
    "BETA_RANGE = (0.79, 0.99)\n",
    "GAMMA_RANGE = (0.05, 0.50)\n",
    "MU_J_RANGE = (-0.05, 0.05)\n",
    "SIGMA_J_RANGE = (0.01, 0.10)\n",
    "LAMBDA_RANGE = (0.005, 0.05)\n",
    "\n",
    "PARAM_NAMES = [\"mju\", \"mjuJ\", \"sigmaJ\", \"varLT\", \"beta\", \"gamma\", \"lambda\"]\n",
    "NUM_RESPONSES = 7\n",
    "\n",
    "# --- LSTM-Transformer Architecture Parameters ---\n",
    "INPUT_FEATURE_DIM = 1 # Number of features per time step (e.g., 1 for raw returns)\n",
    "\n",
    "# LSTM Part\n",
    "LSTM_HIDDEN_SIZE = 64\n",
    "LSTM_NUM_LAYERS = 2\n",
    "LSTM_BIDIRECTIONAL = True\n",
    "LSTM_DROPOUT = 0.1 # Dropout between LSTM layers if num_layers > 1\n",
    "\n",
    "# Dimension of LSTM output features fed to Transformer's input projection\n",
    "LSTM_OUTPUT_FEATURE_DIM = LSTM_HIDDEN_SIZE * (2 if LSTM_BIDIRECTIONAL else 1)\n",
    "\n",
    "# Transformer Part\n",
    "TRANSFORMER_D_MODEL = 64  # Embedding dimension for the Transformer\n",
    "TRANSFORMER_NHEAD = 4     # Number of attention heads (d_model must be divisible by nhead)\n",
    "TRANSFORMER_NUM_ENCODER_LAYERS = 2 # Reduced slightly due to LSTM frontend\n",
    "TRANSFORMER_DIM_FEEDFORWARD = 128 \n",
    "TRANSFORMER_DROPOUT = 0.1\n",
    "PE_DROPOUT = 0.1          # Dropout for positional encoding\n",
    "\n",
    "# Final MLP Part (after Transformer)\n",
    "MLP_NUM_NEURONS_FC = 64 \n",
    "\n",
    "# Training Parameters\n",
    "BATCH_SIZE = 16 # Potentially small due to model size\n",
    "MAX_EPOCHS = 100      \n",
    "LEARNING_RATE = 1e-4 \n",
    "EARLY_STOPPING_PATIENCE = 20\n",
    "LEAKY_RELU_SLOPE = 0.01\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SVJD Data Generation (Vectorized) ---\n",
    "def generate_svjd_data(n_samples, t_length, fixed_params=None):\n",
    "    if fixed_params is not None:\n",
    "        action_desc = \"Generating SVJD Data (Fixed Params)\"\n",
    "        mu_val_arr = np.full(n_samples, fixed_params[0])\n",
    "        mu_j_val_arr = np.full(n_samples, fixed_params[1])\n",
    "        sigma_j_val_arr = np.full(n_samples, fixed_params[2])\n",
    "        v_lt_val_arr = np.full(n_samples, fixed_params[3])\n",
    "        beta_val_arr = np.full(n_samples, fixed_params[4])\n",
    "        gamma_val_arr = np.full(n_samples, fixed_params[5])\n",
    "        lambda_val_arr = np.full(n_samples, fixed_params[6])\n",
    "    else:\n",
    "        action_desc = \"Generating SVJD Data (Sampled Params)\"\n",
    "        mu_val_arr = np.random.uniform(*MU_RANGE, size=n_samples)\n",
    "        v_lt_base_arr = np.random.uniform(np.sqrt(V_LT_RANGE[0]), np.sqrt(V_LT_RANGE[1]), size=n_samples)\n",
    "        v_lt_val_arr = v_lt_base_arr**2\n",
    "        beta_val_arr = np.random.uniform(*BETA_RANGE, size=n_samples)\n",
    "        gamma_val_arr = np.random.uniform(*GAMMA_RANGE, size=n_samples)\n",
    "        mu_j_val_arr = np.random.uniform(*MU_J_RANGE, size=n_samples)\n",
    "        sigma_j_val_arr = np.random.uniform(*SIGMA_J_RANGE, size=n_samples)\n",
    "        lambda_val_arr = np.random.uniform(*LAMBDA_RANGE, size=n_samples)\n",
    "\n",
    "    all_params_np = np.column_stack((mu_val_arr, mu_j_val_arr, sigma_j_val_arr, \n",
    "                                     v_lt_val_arr, beta_val_arr, gamma_val_arr, lambda_val_arr))\n",
    "    alpha_val_arr = np.log(v_lt_val_arr) * (1 - beta_val_arr)\n",
    "    eps_h_all = np.random.normal(0, 1, size=(n_samples, t_length))\n",
    "    eps_r_all = np.random.normal(0, 1, size=(n_samples, t_length))\n",
    "    Z_all = np.random.binomial(1, lambda_val_arr[:, np.newaxis], size=(n_samples, t_length))\n",
    "    J_components_all = np.random.normal(mu_j_val_arr[:, np.newaxis], \n",
    "                                        sigma_j_val_arr[:, np.newaxis], \n",
    "                                        size=(n_samples, t_length))\n",
    "    J_all = J_components_all * Z_all\n",
    "    h_all = np.zeros((n_samples, t_length))\n",
    "    h_all[:, 0] = np.log(v_lt_val_arr)\n",
    "\n",
    "    use_tqdm_ht = n_samples > 100 or fixed_params is None \n",
    "    ht_iterator = tqdm(range(t_length - 1), desc=\"Simulating h_t paths\", leave=False) if use_tqdm_ht else range(t_length - 1)\n",
    "\n",
    "    for t_idx in ht_iterator:\n",
    "        h_all[:, t_idx + 1] = alpha_val_arr + beta_val_arr * h_all[:, t_idx] + gamma_val_arr * eps_h_all[:, t_idx]\n",
    "\n",
    "    sigma_t_sq_all = np.exp(h_all)\n",
    "    sigma_t_all = np.sqrt(sigma_t_sq_all)\n",
    "    returns_all_np = mu_val_arr[:, np.newaxis] + sigma_t_all * eps_r_all + J_all\n",
    "    return torch.tensor(returns_all_np, dtype=torch.float32), torch.tensor(all_params_np, dtype=torch.float32)\n",
    "\n",
    "# --- 2. Data Preprocessing ---\n",
    "def normalize_inputs(data_tensor, mean_val=None, std_val=None):\n",
    "    if mean_val is None or std_val is None: \n",
    "        mean_val = torch.mean(data_tensor) \n",
    "        std_val = torch.std(data_tensor)\n",
    "        if std_val == 0: std_val = 1.0\n",
    "    return (data_tensor - mean_val) / std_val, mean_val, std_val\n",
    "\n",
    "def normalize_outputs(data_tensor, mean_vals=None, std_vals=None):\n",
    "    if mean_vals is None or std_vals is None: \n",
    "        mean_vals = torch.mean(data_tensor, dim=0) \n",
    "        std_vals = torch.std(data_tensor, dim=0)\n",
    "        std_vals[std_vals == 0] = 1.0 \n",
    "    return (data_tensor - mean_vals) / std_vals, mean_vals, std_vals\n",
    "\n",
    "def denormalize_outputs(normalized_data_tensor, mean_vals, std_vals):\n",
    "    return normalized_data_tensor * std_vals + mean_vals\n",
    "\n",
    "# --- 3. LSTM-Transformer Model Definition ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 != 0: \n",
    "             pe[:, 1::2] = torch.cos(position * div_term[:-1]) \n",
    "        else:\n",
    "             pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:x.size(1), :] \n",
    "        return self.dropout(x)\n",
    "\n",
    "class LSTMTransformerSVJD(nn.Module):\n",
    "    def __init__(self, num_input_features=INPUT_FEATURE_DIM, num_responses=NUM_RESPONSES,\n",
    "                 lstm_hidden_size=LSTM_HIDDEN_SIZE,\n",
    "                 lstm_num_layers=LSTM_NUM_LAYERS,\n",
    "                 lstm_bidirectional=LSTM_BIDIRECTIONAL,\n",
    "                 lstm_dropout=LSTM_DROPOUT,\n",
    "                 transformer_d_model=TRANSFORMER_D_MODEL,\n",
    "                 transformer_nhead=TRANSFORMER_NHEAD,\n",
    "                 transformer_num_encoder_layers=TRANSFORMER_NUM_ENCODER_LAYERS,\n",
    "                 transformer_dim_feedforward=TRANSFORMER_DIM_FEEDFORWARD,\n",
    "                 transformer_dropout=TRANSFORMER_DROPOUT,\n",
    "                 pe_dropout=PE_DROPOUT,\n",
    "                 mlp_fc_neurons=MLP_NUM_NEURONS_FC,\n",
    "                 leaky_slope=LEAKY_RELU_SLOPE,\n",
    "                 max_seq_len_for_pe=T_LENGTH\n",
    "                 ):\n",
    "        super(LSTMTransformerSVJD, self).__init__()\n",
    "        \n",
    "        self.lstm_output_dim = lstm_hidden_size * (2 if lstm_bidirectional else 1)\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_input_features,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_num_layers,\n",
    "            bidirectional=lstm_bidirectional,\n",
    "            dropout=lstm_dropout if lstm_num_layers > 1 else 0, # Dropout only between layers\n",
    "            batch_first=True # Input: (batch, seq, feature)\n",
    "        )\n",
    "        \n",
    "        # Linear layer to project LSTM output to Transformer's d_model\n",
    "        self.lstm_to_transformer_projection = nn.Linear(self.lstm_output_dim, transformer_d_model)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(transformer_d_model, pe_dropout, max_len=max_seq_len_for_pe)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=transformer_d_model, \n",
    "            nhead=transformer_nhead,\n",
    "            dim_feedforward=transformer_dim_feedforward,\n",
    "            dropout=transformer_dropout,\n",
    "            batch_first=True \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_num_encoder_layers)\n",
    "\n",
    "        # Final MLP\n",
    "        self.fc1_mlp = nn.Linear(transformer_d_model, mlp_fc_neurons)\n",
    "        self.relu_fc1_mlp = nn.LeakyReLU(leaky_slope)\n",
    "        self.ln_fc1_mlp = nn.LayerNorm(mlp_fc_neurons)\n",
    "        \n",
    "        self.fc2_mlp = nn.Linear(mlp_fc_neurons, mlp_fc_neurons // 2)\n",
    "        self.relu_fc2_mlp = nn.LeakyReLU(leaky_slope)\n",
    "        self.ln_fc2_mlp = nn.LayerNorm(mlp_fc_neurons // 2)\n",
    "\n",
    "        self.fc_out_mlp = nn.Linear(mlp_fc_neurons // 2, num_responses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_input_features, sequence_length)\n",
    "        # e.g., (batch_size, 1, 2000)\n",
    "        if x.ndim == 2: # (batch_size, sequence_length) -> assumes num_input_features=1\n",
    "            x = x.unsqueeze(1) # Add feature dimension\n",
    "\n",
    "        x = x.permute(0, 2, 1) # (batch_size, sequence_length, num_input_features) for LSTM\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        # lstm_out shape: (batch_size, sequence_length, lstm_output_dim)\n",
    "        \n",
    "        # Project LSTM output to Transformer d_model\n",
    "        transformer_input = self.lstm_to_transformer_projection(lstm_out)\n",
    "        # transformer_input shape: (batch_size, sequence_length, transformer_d_model)\n",
    "        \n",
    "        transformer_input = self.pos_encoder(transformer_input)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        transformer_out = self.transformer_encoder(transformer_input)\n",
    "        # transformer_out shape: (batch_size, sequence_length, transformer_d_model)\n",
    "        \n",
    "        # Aggregate Transformer output - e.g., mean pooling over sequence dimension\n",
    "        aggregated_out = transformer_out.mean(dim=1) # (batch_size, transformer_d_model)\n",
    "        \n",
    "        # Final MLP\n",
    "        mlp_out = self.fc1_mlp(aggregated_out); mlp_out = self.relu_fc1_mlp(mlp_out); mlp_out = self.ln_fc1_mlp(mlp_out)\n",
    "        mlp_out = self.fc2_mlp(mlp_out); mlp_out = self.relu_fc2_mlp(mlp_out); mlp_out = self.ln_fc2_mlp(mlp_out)\n",
    "        final_out = self.fc_out_mlp(mlp_out)\n",
    "        \n",
    "        return final_out\n",
    "\n",
    "# --- 4. R-squared Calculation ---\n",
    "def calculate_r2_score(y_true, y_pred):\n",
    "    ss_res = torch.sum((y_true - y_pred)**2, dim=0)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true, dim=0))**2, dim=0)\n",
    "    return 1 - ss_res / (ss_tot + 1e-8) \n",
    "\n",
    "# --- 5. Training Loop ---\n",
    "def train_model(model, train_loader, val_loader, test_loader_norm, test_params_raw,\n",
    "                criterion, optimizer, num_epochs, patience, device,\n",
    "                param_output_means, param_output_stds, param_names_list):\n",
    "    print(f\"Starting training with model: {model.__class__.__name__}\")\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'test_r2': {name: [] for name in param_names_list}}\n",
    "    model_save_path = f'best_svjd_{model.__class__.__name__.lower()}_model.pth'\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time_epoch = time.time()\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * inputs.size(0)\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        if val_loader:\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    running_val_loss += loss.item() * inputs.size(0)\n",
    "            epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "            history['val_loss'].append(epoch_val_loss)\n",
    "        else:\n",
    "            epoch_val_loss = float('nan') \n",
    "            history['val_loss'].append(epoch_val_loss)\n",
    "        \n",
    "        all_test_preds_denorm = []\n",
    "        r2_scores_dict = {name: float('nan') for name in param_names_list} \n",
    "        if test_loader_norm and test_params_raw is not None:\n",
    "            with torch.no_grad():\n",
    "                for test_inputs_batch in tqdm(test_loader_norm, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\", leave=False):\n",
    "                    test_inputs_batch = test_inputs_batch[0].to(device) \n",
    "                    preds_norm = model(test_inputs_batch)\n",
    "                    preds_denorm = denormalize_outputs(preds_norm.cpu(), param_output_means, param_output_stds)\n",
    "                    all_test_preds_denorm.append(preds_denorm)\n",
    "            \n",
    "            if all_test_preds_denorm:\n",
    "                all_test_preds_denorm_tensor = torch.cat(all_test_preds_denorm, dim=0)\n",
    "                r2_scores_test = calculate_r2_score(test_params_raw.cpu(), all_test_preds_denorm_tensor)\n",
    "                r2_scores_dict = {name: r2_scores_test[i].item() for i, name in enumerate(param_names_list)}\n",
    "                for name, r2_val in r2_scores_dict.items():\n",
    "                    history['test_r2'][name].append(r2_val)\n",
    "\n",
    "        end_time_epoch = time.time()\n",
    "        epoch_duration = end_time_epoch - start_time_epoch\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_train_loss:.6f}, Val Loss: {epoch_val_loss:.6f}, Duration: {epoch_duration:.2f}s\")\n",
    "        if test_loader_norm and all_test_preds_denorm: \n",
    "            r2_report_str = \", \".join([f\"R2({name}): {score:.4f}\" for name, score in r2_scores_dict.items()])\n",
    "            print(f\"Test R2 Scores: {r2_report_str}\")\n",
    "\n",
    "        if val_loader and epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_save_path) \n",
    "        elif val_loader:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if val_loader and epochs_no_improve >= patience: \n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement on validation set.\")\n",
    "            break\n",
    "            \n",
    "    print(\"Training finished.\")\n",
    "    if val_loader and best_val_loss != float('inf'): \n",
    "        print(f\"Loading best model weights from {model_save_path} based on validation loss...\")\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(model_save_path))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: '{model_save_path}' not found. Using last model state.\")\n",
    "    elif not val_loader:\n",
    "        print(\"No validation loader provided, using last model state.\")\n",
    "    else: \n",
    "        print(\"Best validation loss was not updated, using last model state.\")\n",
    "        \n",
    "    return model, history\n",
    "\n",
    "# --- 6. Evaluation Function for Fixed Parameters ---\n",
    "def evaluate_model_on_fixed_params(trained_model, fixed_params_vector, N_eval_samples, t_length, \n",
    "                                   returns_norm_mean_stat, returns_norm_std_stat, \n",
    "                                   params_norm_means_stat, params_norm_stds_stat, \n",
    "                                   device, param_names_list):\n",
    "    print(f\"\\n--- Evaluating model {trained_model.__class__.__name__} on {N_eval_samples} samples with fixed parameters ---\")\n",
    "    if isinstance(fixed_params_vector, torch.Tensor):\n",
    "        fixed_params_vector = fixed_params_vector.cpu().numpy().tolist()\n",
    "    elif isinstance(fixed_params_vector, np.ndarray):\n",
    "        fixed_params_vector = fixed_params_vector.tolist()\n",
    "\n",
    "    eval_returns_raw, eval_params_true_raw = generate_svjd_data(\n",
    "        n_samples=N_eval_samples, t_length=t_length, fixed_params=fixed_params_vector\n",
    "    )\n",
    "    eval_returns_norm, _, _ = normalize_inputs(eval_returns_raw, \n",
    "                                               returns_norm_mean_stat, returns_norm_std_stat)\n",
    "    trained_model.eval()\n",
    "    all_predictions_denormalized = []\n",
    "    eval_dataset_norm_inputs_only = TensorDataset(eval_returns_norm)\n",
    "    eval_loader_norm = DataLoader(eval_dataset_norm_inputs_only, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs_batch in tqdm(eval_loader_norm, desc=\"Predicting on fixed-param samples\"):\n",
    "            inputs_batch = inputs_batch[0].to(device)\n",
    "            predicted_params_norm = trained_model(inputs_batch)\n",
    "            predicted_params_denorm = denormalize_outputs(\n",
    "                predicted_params_norm.cpu(), params_norm_means_stat, params_norm_stds_stat\n",
    "            )\n",
    "            all_predictions_denormalized.append(predicted_params_denorm)\n",
    "    if not all_predictions_denormalized: return pd.DataFrame()\n",
    "    all_predictions_tensor = torch.cat(all_predictions_denormalized, dim=0)\n",
    "    df_data = {}\n",
    "    for i, name in enumerate(param_names_list):\n",
    "        df_data[f'{name}_true'] = eval_params_true_raw[:, i].cpu().numpy()\n",
    "        df_data[f'{name}_pred'] = all_predictions_tensor[:, i].cpu().numpy()\n",
    "    results_df = pd.DataFrame(df_data)\n",
    "    print(f\"--- Evaluation with fixed parameters complete. ---\")\n",
    "    return results_df\n",
    "\n",
    "# --- 7. Test Parameter Range Function ---\n",
    "def test_parameter_range(trained_model, param_to_test_name, param_min_val, param_max_val, num_steps,\n",
    "                         base_params_dict, N_series_per_step, t_length,\n",
    "                         returns_norm_mean_stat, returns_norm_std_stat,\n",
    "                         params_norm_means_stat, params_norm_stds_stat,\n",
    "                         device, all_param_names_list):\n",
    "    print(f\"\\n--- Testing {trained_model.__class__.__name__} for '{param_to_test_name}' from {param_min_val:.4f} to {param_max_val:.4f} ---\")\n",
    "    param_values_to_test = np.linspace(param_min_val, param_max_val, num_steps)\n",
    "    results_list = []\n",
    "    try:\n",
    "        param_idx_to_test = all_param_names_list.index(param_to_test_name)\n",
    "    except ValueError: return pd.DataFrame() \n",
    "    base_full_params_ordered = [base_params_dict.get(name, 0.0) for name in all_param_names_list]\n",
    "\n",
    "    for test_val in tqdm(param_values_to_test, desc=f\"Testing range for {param_to_test_name}\"):\n",
    "        current_full_params = list(base_full_params_ordered)\n",
    "        current_full_params[param_idx_to_test] = test_val\n",
    "        series_batch_raw, _ = generate_svjd_data(\n",
    "            n_samples=N_series_per_step, t_length=t_length, fixed_params=current_full_params\n",
    "        )\n",
    "        series_batch_norm, _, _ = normalize_inputs(series_batch_raw, returns_norm_mean_stat, returns_norm_std_stat)\n",
    "        trained_model.eval()\n",
    "        batch_predictions_denorm_list = []\n",
    "        temp_dataset = TensorDataset(series_batch_norm)\n",
    "        temp_loader = DataLoader(temp_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs_batch_eval in temp_loader:\n",
    "                inputs_eval = inputs_batch_eval[0].to(device)\n",
    "                preds_norm_eval = trained_model(inputs_eval)\n",
    "                preds_denorm_eval = denormalize_outputs(preds_norm_eval.cpu(), params_norm_means_stat, params_norm_stds_stat)\n",
    "                batch_predictions_denorm_list.append(preds_denorm_eval)\n",
    "        if not batch_predictions_denorm_list: continue\n",
    "        batch_predictions_denorm = torch.cat(batch_predictions_denorm_list, dim=0)\n",
    "        avg_prediction_for_step = torch.mean(batch_predictions_denorm, dim=0).numpy()\n",
    "        row_data = {f'{param_to_test_name}_true': test_val}\n",
    "        for i, name in enumerate(all_param_names_list):\n",
    "            row_data[f'{name}_pred'] = avg_prediction_for_step[i]\n",
    "        results_list.append(row_data)\n",
    "    if not results_list: return pd.DataFrame()\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    print(f\"--- Parameter range testing for '{param_to_test_name}' complete. ---\")\n",
    "    return results_df\n",
    "\n",
    "# --- 8. Main Script Execution ---\n",
    "if __name__ == '__main__':\n",
    "    # Quick test settings\n",
    "    # actual_n_train_val = 1000 \n",
    "    # actual_n_test = 50       \n",
    "    # actual_max_epochs = 3 \n",
    "    # N_for_fixed_eval = 5\n",
    "    # num_sweep_steps = 5\n",
    "    # current_batch_size = BATCH_SIZE \n",
    "\n",
    "    # Full run settings\n",
    "    actual_n_train_val = N_SAMPLES_TOTAL\n",
    "    actual_n_test = N_TEST_SAMPLES\n",
    "    actual_max_epochs = MAX_EPOCHS\n",
    "    N_for_fixed_eval = 100 \n",
    "    num_sweep_steps = 10   \n",
    "    current_batch_size = BATCH_SIZE\n",
    "\n",
    "\n",
    "    print(f\"--- Generating Training/Validation Data ({actual_n_train_val} samples) ---\")\n",
    "    train_val_returns_raw, train_val_params_raw = generate_svjd_data(actual_n_train_val, T_LENGTH)\n",
    "    print(f\"\\n--- Generating Test Data ({actual_n_test} samples) ---\")\n",
    "    test_returns_raw, test_params_raw = generate_svjd_data(actual_n_test, T_LENGTH)\n",
    "\n",
    "    n_val_samples = int(actual_n_train_val * VAL_SPLIT_RATIO)\n",
    "    n_train_samples = actual_n_train_val - n_val_samples\n",
    "    train_val_dataset = TensorDataset(train_val_returns_raw, train_val_params_raw)\n",
    "    train_dataset_raw_split, val_dataset_raw_split = random_split(train_val_dataset, [n_train_samples, n_val_samples])\n",
    "    train_returns_for_norm_stats = torch.stack([train_dataset_raw_split[i][0] for i in range(len(train_dataset_raw_split))])\n",
    "    train_params_for_norm_stats = torch.stack([train_dataset_raw_split[i][1] for i in range(len(train_dataset_raw_split))])\n",
    "\n",
    "    print(\"\\n--- Normalizing Data (using training split statistics) ---\")\n",
    "    train_returns_norm, returns_mean_stat, returns_std_stat = normalize_inputs(train_returns_for_norm_stats)\n",
    "    train_params_norm, params_means_stat, params_stds_stat = normalize_outputs(train_params_for_norm_stats)\n",
    "    print(f\"Return series mean: {returns_mean_stat.item():.6f}, std: {returns_std_stat.item():.6f}\")\n",
    "    print(f\"Parameter means: {[f'{m:.4f}' for m in params_means_stat.tolist()]}\")\n",
    "    print(f\"Parameter stds: {[f'{s:.4f}' for s in params_stds_stat.tolist()]}\")\n",
    "\n",
    "    val_loader = None\n",
    "    if len(val_dataset_raw_split) > 0:\n",
    "        val_returns_list_raw = [val_dataset_raw_split[i][0] for i in range(len(val_dataset_raw_split))]\n",
    "        val_params_list_raw = [val_dataset_raw_split[i][1] for i in range(len(val_dataset_raw_split))]\n",
    "        val_returns_tensor_raw = torch.stack(val_returns_list_raw)\n",
    "        val_params_tensor_raw = torch.stack(val_params_list_raw)\n",
    "        val_returns_norm, _, _ = normalize_inputs(val_returns_tensor_raw, returns_mean_stat, returns_std_stat)\n",
    "        val_params_norm, _, _ = normalize_outputs(val_params_tensor_raw, params_means_stat, params_stds_stat)\n",
    "        val_dataset_norm = TensorDataset(val_returns_norm, val_params_norm)\n",
    "        val_loader = DataLoader(val_dataset_norm, batch_size=current_batch_size, shuffle=False)\n",
    "\n",
    "    test_returns_norm, _, _ = normalize_inputs(test_returns_raw, returns_mean_stat, returns_std_stat)\n",
    "    train_dataset_norm = TensorDataset(train_returns_norm, train_params_norm)\n",
    "    train_loader = DataLoader(train_dataset_norm, batch_size=current_batch_size, shuffle=True)\n",
    "    test_dataset_norm_inputs_only = TensorDataset(test_returns_norm) \n",
    "    test_loader_norm = DataLoader(test_dataset_norm_inputs_only, batch_size=current_batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nTrain loader: {len(train_loader.dataset)} samples. Val loader: {len(val_loader.dataset) if val_loader else 0}. Test loader: {len(test_loader_norm.dataset)}.\")\n",
    "\n",
    "    # Instantiate the NEW LSTM-Transformer model\n",
    "    lstm_transformer_model = LSTMTransformerSVJD(\n",
    "        num_input_features=INPUT_FEATURE_DIM, \n",
    "        num_responses=NUM_RESPONSES,\n",
    "        max_seq_len_for_pe= T_LENGTH + 10 \n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.MSELoss() \n",
    "    optimizer = optim.Adam(lstm_transformer_model.parameters(), lr=LEARNING_RATE) \n",
    "\n",
    "    trained_model, history = train_model(lstm_transformer_model, train_loader, val_loader, test_loader_norm, test_params_raw,\n",
    "                                         criterion, optimizer, actual_max_epochs, EARLY_STOPPING_PATIENCE, DEVICE,\n",
    "                                         params_means_stat, params_stds_stat, PARAM_NAMES)\n",
    "    \n",
    "    print(f\"\\n--- Final R2 Scores on Test Set ({trained_model.__class__.__name__}) ---\")\n",
    "    all_test_preds_denorm_final = []\n",
    "    trained_model.eval() \n",
    "    with torch.no_grad():\n",
    "        for inputs_final_test_batch in tqdm(test_loader_norm, desc=\"Final Test Evaluation\", leave=False):\n",
    "            inputs_final_test = inputs_final_test_batch[0].to(DEVICE)\n",
    "            preds_norm_final = trained_model(inputs_final_test)\n",
    "            preds_denorm_final = denormalize_outputs(preds_norm_final.cpu(), params_means_stat, params_stds_stat)\n",
    "            all_test_preds_denorm_final.append(preds_denorm_final)\n",
    "    if all_test_preds_denorm_final:\n",
    "        all_test_preds_denorm_tensor_final = torch.cat(all_test_preds_denorm_final, dim=0)\n",
    "        final_r2_scores_test = calculate_r2_score(test_params_raw.cpu(), all_test_preds_denorm_tensor_final)\n",
    "        final_r2_scores_dict = {name: final_r2_scores_test[i].item() for i, name in enumerate(PARAM_NAMES)}\n",
    "        final_r2_report_str = \", \".join([f\"R2({name}): {score:.4f}\" for name, score in final_r2_scores_dict.items()])\n",
    "        print(final_r2_report_str)\n",
    "\n",
    "    example_fixed_params_list = [0.0001, 0.01, 0.05, 0.0002, 0.95, 0.2, 0.02]\n",
    "    if 'trained_model' in locals() and 'returns_mean_stat' in locals():\n",
    "        fixed_param_eval_df = evaluate_model_on_fixed_params(\n",
    "            trained_model=trained_model, fixed_params_vector=example_fixed_params_list,\n",
    "            N_eval_samples=N_for_fixed_eval, t_length=T_LENGTH,\n",
    "            returns_norm_mean_stat=returns_mean_stat, returns_norm_std_stat=returns_std_stat,\n",
    "            params_norm_means_stat=params_means_stat, params_norm_stds_stat=params_stds_stat,\n",
    "            device=DEVICE, param_names_list=PARAM_NAMES\n",
    "        )\n",
    "        print(f\"\\nDataFrame from fixed parameter evaluation ({trained_model.__class__.__name__}, first 5 rows):\")\n",
    "        print(fixed_param_eval_df.head())\n",
    "\n",
    "    base_params_for_range_test_dict = dict(zip(PARAM_NAMES, example_fixed_params_list))\n",
    "    param_to_sweep = \"gamma\" \n",
    "    param_range = GAMMA_RANGE\n",
    "    N_series_per_sweep_step = 1 \n",
    "\n",
    "    if 'trained_model' in locals() and 'returns_mean_stat' in locals():\n",
    "        range_test_df = test_parameter_range(\n",
    "            trained_model=trained_model, param_to_test_name=param_to_sweep,\n",
    "            param_min_val=param_range[0], param_max_val=param_range[1], num_steps=num_sweep_steps,\n",
    "            base_params_dict=base_params_for_range_test_dict, N_series_per_step=N_series_per_sweep_step,\n",
    "            t_length=T_LENGTH, returns_norm_mean_stat=returns_mean_stat, returns_norm_std_stat=returns_std_stat,\n",
    "            params_norm_means_stat=params_means_stat, params_norm_stds_stat=params_stds_stat,\n",
    "            device=DEVICE, all_param_names_list=PARAM_NAMES\n",
    "        )\n",
    "        print(f\"\\nDataFrame from '{param_to_sweep}' range test ({trained_model.__class__.__name__}, first 5 rows):\")\n",
    "        print(range_test_df.head())\n",
    "\n",
    "    print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing LSTMTransformerSVJD for 'mju' from -0.0004 to 0.0004 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing range for mju: 100%|██████████| 1000/1000 [00:09<00:00, 108.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Parameter range testing for 'mju' complete. ---\n",
      "   mju_true  mju_pred  mjuJ_pred  sigmaJ_pred  varLT_pred  beta_pred  \\\n",
      "0 -0.000400 -0.000184   0.030698     0.052928    0.000201   0.919871   \n",
      "1 -0.000399  0.000141   0.014573     0.058084    0.000200   0.938543   \n",
      "2 -0.000398 -0.000256   0.012322     0.069533    0.000167   0.941327   \n",
      "3 -0.000398 -0.000287   0.018034     0.050079    0.000185   0.916569   \n",
      "4 -0.000397 -0.000238   0.006128     0.054915    0.000191   0.964074   \n",
      "\n",
      "   gamma_pred  lambda_pred  \n",
      "0    0.191153     0.017710  \n",
      "1    0.203676     0.024679  \n",
      "2    0.206462     0.019621  \n",
      "3    0.236742     0.019946  \n",
      "4    0.197249     0.020528  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "param_to_sweep = \"mju\" # Example: sweep the 'beta' parameter\n",
    "beta_min, beta_max = MU_RANGE[0], MU_RANGE[1] # Use defined range for beta\n",
    "num_sweep_steps = 1000 # Number of points in the sweep (e.g., 1000 is too much for quick test)\n",
    "N_series_per_sweep_step = 1 # Number of series to generate for each beta value\n",
    "\n",
    "if 'trained_model' in locals() and \\\n",
    "    'returns_mean_stat' in locals() and 'params_means_stat' in locals() :\n",
    "    \n",
    "    range_test_df = test_parameter_range(\n",
    "        trained_model=trained_model,\n",
    "        param_to_test_name=param_to_sweep,\n",
    "        param_min_val=beta_min,\n",
    "        param_max_val=beta_max,\n",
    "        num_steps=num_sweep_steps,\n",
    "        base_params_dict=base_params_for_range_test_dict,\n",
    "        N_series_per_step=N_series_per_sweep_step,\n",
    "        t_length=T_LENGTH,\n",
    "        returns_norm_mean_stat=returns_mean_stat,\n",
    "        returns_norm_std_stat=returns_std_stat,\n",
    "        params_norm_means_stat=params_means_stat,\n",
    "        params_norm_stds_stat=params_stds_stat,\n",
    "        device=DEVICE,\n",
    "        all_param_names_list=PARAM_NAMES\n",
    "    )\n",
    "    print(range_test_df.head())\n",
    "    # range_test_df.to_csv(f\"{param_to_sweep}_range_test_results.csv\", index=False)\n",
    "    # print(f\"\\nSaved {param_to_sweep} range test results.\")\n",
    "else:\n",
    "    print(\"\\nSkipping parameter range testing as model/stats are not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f44531b0970>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "range_test_df.plot.scatter(x='mju_true', y='mju_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4454de3190>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_param_eval_df['mju_pred'].sub(fixed_param_eval_df['mju_true']).div(fixed_param_eval_df['mju_true']).plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 22:23:39,323 INFO Starting SVJD CNN training\n",
      "2025-05-20 22:23:39,323 INFO Starting SVJD CNN training\n",
      "2025-05-20 22:23:39,323 INFO Starting SVJD CNN training\n",
      "2025-05-20 22:23:39,323 INFO Starting SVJD CNN training\n",
      "2025-05-20 22:23:39,323 INFO Starting SVJD CNN training\n",
      "2025-05-20 22:23:39,324 INFO Simulating training returns...\n",
      "2025-05-20 22:23:39,324 INFO Simulating training returns...\n",
      "2025-05-20 22:23:39,324 INFO Simulating training returns...\n",
      "2025-05-20 22:23:39,324 INFO Simulating training returns...\n",
      "2025-05-20 22:23:39,324 INFO Simulating training returns...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[20, 398], expected input with shape [*, 20, 398], but got input of size[50, 20, 399]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 196\u001b[0m\n\u001b[1;32m    194\u001b[0m model \u001b[38;5;241m=\u001b[39m SVJD_CNN(T)\n\u001b[1;32m    195\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 196\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[1;32m    199\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimulating test returns and evaluating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 129\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, max_epochs, patience, lr, device)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Xb, Yb \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    128\u001b[0m     Xb, Yb \u001b[38;5;241m=\u001b[39m Xb\u001b[38;5;241m.\u001b[39mto(device), Yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 129\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(pred, Yb)\n\u001b[1;32m    131\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/sbt/svcj/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sbt/svcj/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 112\u001b[0m, in \u001b[0;36mSVJD_CNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[1;32m    111\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[0;32m--> 112\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_pool(x)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/sbt/svcj/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sbt/svcj/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/sbt/svcj/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/sbt/svcj/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sbt/svcj/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/sbt/svcj/.venv/lib/python3.8/site-packages/torch/nn/modules/normalization.py:202\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sbt/svcj/.venv/lib/python3.8/site-packages/torch/nn/functional.py:2576\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2574\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2575\u001b[0m     )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[20, 398], expected input with shape [*, 20, 398], but got input of size[50, 20, 399]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import r2_score\n",
    "import logging\n",
    "\n",
    "# Configure logging: write to file and console\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s',\n",
    "                    filename='training.log',\n",
    "                    filemode='w')\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger('').addHandler(console)\n",
    "\n",
    "# 1) SVJD simulation ----------------------------------------------------------\n",
    "def simulate_svjd(mu, v_lt, beta, gamma, mu_j, sigma_j, lam, T, seed=None):\n",
    "    \"\"\"\n",
    "    Simulate T returns from the SVJD model:\n",
    "      h_t = alpha + beta * h_{t-1} + gamma * eps_h\n",
    "      r_t = mu + exp(h_t/2) * eps_r + Z_t * J_t\n",
    "    where\n",
    "      alpha = ln(v_lt) * (1 - beta)\n",
    "      eps_r, eps_h ~ N(0,1)\n",
    "      Z_t ~ Bernoulli(lam)\n",
    "      J_t ~ N(mu_j, sigma_j)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    alpha = np.log(v_lt) * (1 - beta)\n",
    "    h = np.empty(T, dtype=np.float32)\n",
    "    r = np.empty(T, dtype=np.float32)\n",
    "    h[0] = alpha / (1 - beta)\n",
    "    eps_h = np.random.randn(T).astype(np.float32)\n",
    "    eps_r = np.random.randn(T).astype(np.float32)\n",
    "    jumps = np.random.binomial(1, lam, size=T)\n",
    "    J = mu_j + sigma_j * np.random.randn(T).astype(np.float32)\n",
    "    for t in range(1, T):\n",
    "        h[t] = alpha + beta * h[t-1] + gamma * eps_h[t]\n",
    "    sigma = np.exp(h / 2)\n",
    "    r = mu + sigma * eps_r + jumps * J\n",
    "    return r\n",
    "\n",
    "# 2) Dataset and normalization ------------------------------------------------\n",
    "class SVJDDataset(Dataset):\n",
    "    def __init__(self, returns, thetas, x_mean, x_std, y_mean, y_std):\n",
    "        self.X = (returns - x_mean) / x_std\n",
    "        self.Y = (thetas - y_mean) / y_std\n",
    "        self.X = torch.from_numpy(self.X).unsqueeze(1).float()\n",
    "        self.Y = torch.from_numpy(self.Y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# 3) Causal conv + CNN model --------------------------------------------------\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ConstantPad1d((kernel_size-1, 0), 0.0)\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(self.pad(x))\n",
    "\n",
    "class SVJD_CNN(nn.Module):\n",
    "    def __init__(self, T, num_responses=7, num_filters=20, num_neurons=20, kernel_size=5, pool_size=5):\n",
    "        super().__init__()\n",
    "        # Conv block 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            CausalConv1d(1, num_filters, kernel_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm([num_filters, T]),\n",
    "            nn.AvgPool1d(kernel_size=pool_size, stride=1)\n",
    "        )\n",
    "        L1 = T - (pool_size - 1)\n",
    "        # Conv block 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            CausalConv1d(num_filters, num_filters, kernel_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm([num_filters, L1]),\n",
    "            nn.AvgPool1d(kernel_size=pool_size, stride=pool_size)\n",
    "        )\n",
    "        L2 = (L1 - (pool_size - 1)) // pool_size\n",
    "        # Conv block 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            CausalConv1d(num_filters, num_filters, kernel_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm([num_filters, L2])\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        # Fully connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_filters, num_neurons),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(num_neurons),\n",
    "            nn.Linear(num_neurons, num_neurons),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LayerNorm(num_neurons),\n",
    "            nn.Linear(num_neurons, num_responses)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# 4) Training with early stopping ------------------------------------------------\n",
    "def train_model(model, train_loader, val_loader, max_epochs=100, patience=50, lr=1e-3, device='cpu'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        train_loss_accum = 0.0\n",
    "        for Xb, Yb in train_loader:\n",
    "            Xb, Yb = Xb.to(device), Yb.to(device)\n",
    "            pred = model(Xb)\n",
    "            loss = criterion(pred, Yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_accum += loss.item() * Xb.size(0)\n",
    "        train_loss = train_loss_accum / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_accum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for Xv, Yv in val_loader:\n",
    "                Xv, Yv = Xv.to(device), Yv.to(device)\n",
    "                val_loss_accum += criterion(model(Xv), Yv).item() * Xv.size(0)\n",
    "        val_loss = val_loss_accum / len(val_loader.dataset)\n",
    "\n",
    "        logging.info(f\"Epoch {epoch}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            epochs_no_improve = 0\n",
    "            logging.info(f\"New best model saved at epoch {epoch} (Val Loss = {val_loss:.6f})\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                logging.info(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    logging.info(f\"Training complete. Best validation loss = {best_val:.6f}\")\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model\n",
    "\n",
    "# 5) Main execution -------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Starting SVJD CNN training\")\n",
    "    N_train, N_test = 500, 500\n",
    "    T = 2000\n",
    "\n",
    "    # Draw training parameters\n",
    "    mus    = np.random.uniform(-0.1, 0.1, N_train).astype(np.float32) / 250\n",
    "    v_lts  = np.random.uniform(0.005, 0.015, N_train).astype(np.float32)**2\n",
    "    betas  = np.random.uniform(0.79, 0.99, N_train).astype(np.float32)\n",
    "    gammas = np.random.uniform(0.05, 0.50, N_train).astype(np.float32)\n",
    "    muj    = np.random.uniform(-0.05, 0.05, N_train).astype(np.float32)\n",
    "    sigmaj = np.random.uniform(0.01, 0.10, N_train).astype(np.float32)\n",
    "    lamb   = np.random.uniform(0.005, 0.05, N_train).astype(np.float32)\n",
    "\n",
    "    logging.info(\"Simulating training returns...\")\n",
    "    R_train = np.vstack([\n",
    "        simulate_svjd(mu, vlt, b, g, mj, sj, lm, T)\n",
    "        for mu, vlt, b, g, mj, sj, lm in zip(mus, v_lts, betas, gammas, muj, sigmaj, lamb)\n",
    "    ])\n",
    "    Y_train = np.column_stack([mus, muj, sigmaj, v_lts, betas, gammas, lamb])\n",
    "\n",
    "    # Normalization\n",
    "    x_mean, x_std = R_train.mean(), R_train.std()\n",
    "    y_mean, y_std = Y_train.mean(0), Y_train.std(0)\n",
    "\n",
    "    dataset = SVJDDataset(R_train, Y_train, x_mean, x_std, y_mean, y_std)\n",
    "    n_val = int(0.2 * len(dataset))\n",
    "    train_ds, val_ds = random_split(dataset, [len(dataset)-n_val, n_val])\n",
    "    train_loader = DataLoader(train_ds, batch_size=50, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=50)\n",
    "\n",
    "    model = SVJD_CNN(T)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = train_model(model, train_loader, val_loader, device=device)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    logging.info(\"Simulating test returns and evaluating model...\")\n",
    "    mus_t    = np.random.uniform(-0.1, 0.1, N_test).astype(np.float32) / 250\n",
    "    v_lts_t  = np.random.uniform(0.005, 0.015, N_test).astype(np.float32)**2\n",
    "    betas_t  = np.random.uniform(0.79, 0.99, N_test).astype(np.float32)\n",
    "    gammas_t = np.random.uniform(0.05, 0.50, N_test).astype(np.float32)\n",
    "    muj_t    = np.random.uniform(-0.05, 0.05, N_test).astype(np.float32)\n",
    "    sigmaj_t = np.random.uniform(0.01, 0.10, N_test).astype(np.float32)\n",
    "    lamb_t   = np.random.uniform(0.005, 0.05, N_test).astype(np.float32)\n",
    "\n",
    "    R_test = np.vstack([\n",
    "        simulate_svjd(mu, vlt, b, g, mj, sj, lm, T)\n",
    "        for mu, vlt, b, g, mj, sj, lm in zip(mus_t, v_lts_t, betas_t, gammas_t, muj_t, sigmaj_t, lamb_t)\n",
    "    ])\n",
    "    Y_test = np.column_stack([mus_t, muj_t, sigmaj_t, v_lts_t, betas_t, gammas_t, lamb_t])\n",
    "\n",
    "    # Normalize and predict\n",
    "    X_test = torch.from_numpy((R_test - x_mean) / x_std).unsqueeze(1).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Y_pred_norm = model(X_test).cpu().numpy()\n",
    "    Y_pred = Y_pred_norm * y_std[None, :] + y_mean[None, :]\n",
    "\n",
    "    # Log R^2 for each parameter\n",
    "    for i, name in enumerate([\"mu\",\"muJ\",\"sigmaJ\",\"vLT\",\"beta\",\"gamma\",\"lambda\"]):\n",
    "        score = r2_score(Y_test[:,i], Y_pred[:,i])\n",
    "        logging.info(f\"Test R^2 for {name}: {score:.4f}\")\n",
    "\n",
    "    logging.info(\"Evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Generating 100000 parameter sets (vectorized)...\n",
      "Simulating 100000 series (vectorized across samples, iterative over T)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5000 parameter sets (vectorized)...\n",
      "Simulating 5000 series (vectorized across samples, iterative over T)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000 parameter sets (vectorized)...\n",
      "Simulating 1000 series (vectorized across samples, iterative over T)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model outputting to runs/svj_transformer_v3_conv_cls_lrwarmup_cap and best_svj_transformer_v3.pth\n",
      "Model initialized. CNN output seq len: 540, Positional encoding max_len: 541\n",
      "Total trainable parameters: 4,847,239\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1 (LR: 2.00e-05): 100%|██████████| 782/782 [05:47<00:00,  2.25it/s, Loss=0.595]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation Results ---\n",
      "Validation Loss (norm): 0.566122\n",
      "Validation R² for mu (denorm): 0.0362\n",
      "Validation R² for v_LT (denorm): 0.8587\n",
      "Validation R² for beta (denorm): 0.2175\n",
      "Validation R² for gamma (denorm): 0.2148\n",
      "Validation R² for mu_J (denorm): 0.7512\n",
      "Validation R² for sigma_J (denorm): 0.6281\n",
      "Validation R² for lambda_J (denorm): 0.3225\n",
      "-------------------------\n",
      "\n",
      "Epoch 1/100 - Time: 352.45s - LR: 2.00e-05 - Train Loss (norm): 0.726170 - Val Loss (norm): 0.566122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2 (LR: 4.00e-05): 100%|██████████| 782/782 [05:58<00:00,  2.18it/s, Loss=0.412]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation Results ---\n",
      "Validation Loss (norm): 0.469295\n",
      "Validation R² for mu (denorm): 0.0456\n",
      "Validation R² for v_LT (denorm): 0.9058\n",
      "Validation R² for beta (denorm): 0.3033\n",
      "Validation R² for gamma (denorm): 0.3323\n",
      "Validation R² for mu_J (denorm): 0.8115\n",
      "Validation R² for sigma_J (denorm): 0.8095\n",
      "Validation R² for lambda_J (denorm): 0.5045\n",
      "-------------------------\n",
      "\n",
      "Epoch 2/100 - Time: 363.01s - LR: 4.00e-05 - Train Loss (norm): 0.483592 - Val Loss (norm): 0.469295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3 (LR: 6.00e-05): 100%|██████████| 782/782 [05:56<00:00,  2.19it/s, Loss=0.364]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation Results ---\n",
      "Validation Loss (norm): 0.325416\n",
      "Validation R² for mu (denorm): 0.4838\n",
      "Validation R² for v_LT (denorm): 0.9312\n",
      "Validation R² for beta (denorm): 0.3708\n",
      "Validation R² for gamma (denorm): 0.5769\n",
      "Validation R² for mu_J (denorm): 0.8419\n",
      "Validation R² for sigma_J (denorm): 0.8724\n",
      "Validation R² for lambda_J (denorm): 0.6321\n",
      "-------------------------\n",
      "\n",
      "Epoch 3/100 - Time: 361.74s - LR: 6.00e-05 - Train Loss (norm): 0.386017 - Val Loss (norm): 0.325416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4 (LR: 8.00e-05):  34%|███▍      | 265/782 [02:00<04:10,  2.06it/s, Loss=0.371]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"T\": 24*90,  # Length of return series (2160)\n",
    "    \"N_TRAIN\": 100_000,\n",
    "    \"N_VAL\": 5000,\n",
    "    \"N_TEST\": 1000,\n",
    "    \"PARAM_NAMES\": ['mu', 'v_LT', 'beta', 'gamma', 'mu_J', 'sigma_J', 'lambda_J'],\n",
    "    \"MODEL_PARAMS\": {\n",
    "        # CNN Stem parameters\n",
    "        \"cnn_out_channels\": [64, 128], # Channels for CNN layers\n",
    "        \"cnn_kernel_sizes\": [7, 5],    # Kernel sizes for CNN layers\n",
    "        \"cnn_strides\": [1, 1],         # Strides for CNN layers (conv)\n",
    "        \"cnn_padding\": [3, 2],         # Padding for CNN layers (conv)\n",
    "        \"cnn_pool_kernel_sizes\": [2, 2], # MaxPool kernel size after each CNN layer\n",
    "        \"cnn_pool_strides\": [2, 2],     # MaxPool stride after each CNN layer\n",
    "\n",
    "        # Transformer parameters\n",
    "        # d_model should match the last cnn_out_channels if cnn_final_proj is False\n",
    "        # Or, cnn_out_channels[-1] will be projected to d_model\n",
    "        \"d_model\": 256,                 # Embedding dimension for Transformer\n",
    "        \"nhead\": 8,                     # Number of attention heads\n",
    "        \"num_encoder_layers\": 6,        # Number of Transformer encoder layers\n",
    "        \"dim_feedforward\": 1024,        # Dimension of feedforward network (e.g., 4 * d_model)\n",
    "        \"dropout\": 0.15,                # Dropout rate in Transformer\n",
    "    },\n",
    "    \"TRAINING_PARAMS\": {\n",
    "        \"batch_size\": 128, # Adjusted for potentially larger model\n",
    "        \"learning_rate\": 1e-4, # Base LR\n",
    "        \"epochs\": 100,\n",
    "        \"early_stopping_patience\": 15, # Increased patience\n",
    "        \"warmup_epochs\": 5, # Number of epochs for LR warmup\n",
    "    },\n",
    "    \"SEED\": 42,\n",
    "    \"TENSORBOARD_LOG_DIR\": \"runs/svj_transformer_v3_conv_cls_lrwarmup_cap\",\n",
    "    \"MODEL_SAVE_PATH\": \"best_svj_transformer_v3.pth\",\n",
    "}\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(CONFIG[\"SEED\"])\n",
    "torch.manual_seed(CONFIG[\"SEED\"])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CONFIG[\"SEED\"])\n",
    "\n",
    "# --- 1. SVJ Model Simulation (Vectorized -UNCHANGED from previous version) ---\n",
    "def generate_svj_parameters_vectorized(num_samples, param_names_list):\n",
    "    params_dict = {}\n",
    "    params_dict['mu'] = np.random.uniform(-0.1, 0.1, size=num_samples) / 250.0\n",
    "    params_dict['v_LT'] = np.random.uniform(0.005, 0.015, size=num_samples)**2\n",
    "    params_dict['beta'] = np.random.uniform(0.79, 0.99, size=num_samples)\n",
    "    params_dict['gamma'] = np.random.uniform(0.05, 0.50, size=num_samples)\n",
    "    params_dict['mu_J'] = np.random.uniform(-0.05, 0.05, size=num_samples)\n",
    "    sigma_J_raw = np.random.uniform(0.01, 0.10, size=num_samples)\n",
    "    params_dict['sigma_J'] = np.maximum(sigma_J_raw, 1e-6)\n",
    "    params_dict['lambda_J'] = np.random.uniform(0.005, 0.05, size=num_samples)\n",
    "    Y = np.stack([params_dict[name] for name in param_names_list], axis=1)\n",
    "    return Y\n",
    "\n",
    "def simulate_svj_series_vectorized(params_array, T, param_names_list):\n",
    "    num_samples = params_array.shape[0]\n",
    "    params_dict = {name: params_array[:, i] for i, name in enumerate(param_names_list)}\n",
    "    mu, v_LT, beta, gamma, mu_J, sigma_J, lambda_J = (\n",
    "        params_dict['mu'], params_dict['v_LT'], params_dict['beta'], params_dict['gamma'],\n",
    "        params_dict['mu_J'], params_dict['sigma_J'], params_dict['lambda_J']\n",
    "    )\n",
    "    v_LT_safe = np.maximum(v_LT, 1e-8)\n",
    "    alpha = np.log(v_LT_safe) * (1 - beta)\n",
    "    returns = np.zeros((num_samples, T))\n",
    "    log_variances = np.zeros((num_samples, T))\n",
    "    log_variances[:, 0] = np.log(v_LT_safe)\n",
    "    for t in tqdm(range(T), desc=\"Simulating time steps\", leave=False):\n",
    "        if t > 0:\n",
    "            eps_h = np.random.normal(0, 1, size=num_samples)\n",
    "            log_variances[:, t] = alpha + beta * log_variances[:, t-1] + gamma * eps_h\n",
    "        else:\n",
    "            log_variances[:, t] = log_variances[:, 0]\n",
    "        sigma_t_sq = np.exp(log_variances[:, t])\n",
    "        sigma_t_sq_safe = np.maximum(sigma_t_sq, 1e-10)\n",
    "        sigma_t = np.sqrt(sigma_t_sq_safe)\n",
    "        Z_t = np.random.binomial(1, lambda_J)\n",
    "        J_t = np.zeros(num_samples)\n",
    "        jump_indices = Z_t > 0\n",
    "        num_active_jumps = np.sum(jump_indices)\n",
    "        if num_active_jumps > 0:\n",
    "            J_t[jump_indices] = np.random.normal(mu_J[jump_indices], sigma_J[jump_indices], size=num_active_jumps)\n",
    "        eps_r = np.random.normal(0, 1, size=num_samples)\n",
    "        returns[:, t] = mu + sigma_t * eps_r + J_t\n",
    "    return returns\n",
    "\n",
    "def generate_dataset_vectorized(num_samples, T, param_names_list):\n",
    "    print(f\"Generating {num_samples} parameter sets (vectorized)...\")\n",
    "    Y_params = generate_svj_parameters_vectorized(num_samples, param_names_list)\n",
    "    print(f\"Simulating {num_samples} series (vectorized across samples, iterative over T)...\")\n",
    "    X_returns = simulate_svj_series_vectorized(Y_params, T, param_names_list)\n",
    "    return torch.tensor(X_returns, dtype=torch.float32), torch.tensor(Y_params, dtype=torch.float32)\n",
    "\n",
    "# --- 2. Dataset and Normalization (UNCHANGED) ---\n",
    "class SVJDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.Y[idx]\n",
    "\n",
    "def normalize_params(params_tensor, means=None, stds=None):\n",
    "    if means is None or stds is None:\n",
    "        means = params_tensor.mean(dim=0, keepdim=True)\n",
    "        stds = params_tensor.std(dim=0, keepdim=True)\n",
    "        stds[stds == 0] = 1\n",
    "    return (params_tensor - means) / stds, means, stds\n",
    "\n",
    "def denormalize_params(normalized_params_tensor, means, stds):\n",
    "    return normalized_params_tensor * stds + means\n",
    "\n",
    "def normalize_returns(returns_tensor, mean=None, std=None):\n",
    "    if mean is None or std is None:\n",
    "        mean = returns_tensor.mean()\n",
    "        std = returns_tensor.std()\n",
    "        if std == 0: std = 1\n",
    "    return (returns_tensor - mean) / std, mean, std\n",
    "\n",
    "# --- 3. Transformer Model (MODIFIED) ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000): # max_len should be dynamic or large enough\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x): # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class SVJTransformer(nn.Module):\n",
    "    def __init__(self, num_params_to_predict, T_input, model_config):\n",
    "        super(SVJTransformer, self).__init__()\n",
    "        \n",
    "        self.d_model = model_config[\"d_model\"]\n",
    "        \n",
    "        # Convolutional Stem\n",
    "        cnn_layers = []\n",
    "        current_channels = 1 # Input is univariate time series\n",
    "        current_T = T_input\n",
    "        \n",
    "        cnn_out_channels_list = model_config[\"cnn_out_channels\"]\n",
    "        cnn_kernel_sizes_list = model_config[\"cnn_kernel_sizes\"]\n",
    "        cnn_strides_list = model_config[\"cnn_strides\"]\n",
    "        cnn_padding_list = model_config[\"cnn_padding\"]\n",
    "        cnn_pool_kernel_sizes_list = model_config[\"cnn_pool_kernel_sizes\"]\n",
    "        cnn_pool_strides_list = model_config[\"cnn_pool_strides\"]\n",
    "\n",
    "        for i in range(len(cnn_out_channels_list)):\n",
    "            cnn_layers.append(nn.Conv1d(\n",
    "                in_channels=current_channels,\n",
    "                out_channels=cnn_out_channels_list[i],\n",
    "                kernel_size=cnn_kernel_sizes_list[i],\n",
    "                stride=cnn_strides_list[i],\n",
    "                padding=cnn_padding_list[i]\n",
    "            ))\n",
    "            cnn_layers.append(nn.ReLU())\n",
    "            # Optional: BatchNorm1d\n",
    "            # cnn_layers.append(nn.BatchNorm1d(cnn_out_channels_list[i]))\n",
    "            cnn_layers.append(nn.MaxPool1d(\n",
    "                kernel_size=cnn_pool_kernel_sizes_list[i],\n",
    "                stride=cnn_pool_strides_list[i]\n",
    "            ))\n",
    "            current_channels = cnn_out_channels_list[i]\n",
    "            # Calculate T after conv\n",
    "            current_T = math.floor((current_T + 2 * cnn_padding_list[i] - (cnn_kernel_sizes_list[i]-1) - 1) / cnn_strides_list[i] + 1)\n",
    "            # Calculate T after pool\n",
    "            current_T = math.floor((current_T - (cnn_pool_kernel_sizes_list[i]-1) -1) / cnn_pool_strides_list[i] + 1)\n",
    "\n",
    "        self.cnn_stem = nn.Sequential(*cnn_layers)\n",
    "        self.cnn_output_seq_len = current_T\n",
    "        \n",
    "        # Projection if CNN output channels don't match d_model for Transformer\n",
    "        if current_channels != self.d_model:\n",
    "            self.cnn_to_transformer_proj = nn.Linear(current_channels, self.d_model)\n",
    "        else:\n",
    "            self.cnn_to_transformer_proj = nn.Identity()\n",
    "\n",
    "        # CLS Token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.d_model)) # Learnable parameter\n",
    "\n",
    "        # Positional Encoding (max_len needs to be cnn_output_seq_len + 1 for CLS)\n",
    "        self.pos_encoder = PositionalEncoding(self.d_model, model_config[\"dropout\"], max_len=self.cnn_output_seq_len + 1)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            self.d_model, model_config[\"nhead\"], model_config[\"dim_feedforward\"],\n",
    "            model_config[\"dropout\"], batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, model_config[\"num_encoder_layers\"])\n",
    "        \n",
    "        # Regression head\n",
    "        self.fc_head = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(model_config[\"dropout\"]), # Add dropout in head too\n",
    "            nn.Linear(self.d_model // 2, num_params_to_predict)\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_len)\n",
    "        src = src.unsqueeze(1) # (batch_size, 1, seq_len) for Conv1d\n",
    "        \n",
    "        # Pass through CNN stem\n",
    "        x = self.cnn_stem(src) # (batch_size, cnn_out_channels[-1], cnn_output_seq_len)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, cnn_output_seq_len, cnn_out_channels[-1])\n",
    "        \n",
    "        # Project to d_model if necessary\n",
    "        x = self.cnn_to_transformer_proj(x) # (batch_size, cnn_output_seq_len, d_model)\n",
    "\n",
    "        # Prepend CLS token\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1) # (batch_size, 1, d_model)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # (batch_size, cnn_output_seq_len + 1, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Pass through Transformer encoder\n",
    "        output = self.transformer_encoder(x) # (batch_size, cnn_output_seq_len + 1, d_model)\n",
    "        \n",
    "        # Use the output of the CLS token for prediction\n",
    "        cls_output = output[:, 0, :] # (batch_size, d_model)\n",
    "        \n",
    "        predictions = self.fc_head(cls_output) # (batch_size, num_params_to_predict)\n",
    "        return predictions\n",
    "\n",
    "# --- 4. Utility Functions (UNCHANGED) ---\n",
    "def r_squared(y_true, y_pred, per_param=True):\n",
    "    if not isinstance(y_true, torch.Tensor): y_true = torch.tensor(y_true)\n",
    "    if not isinstance(y_pred, torch.Tensor): y_pred = torch.tensor(y_pred)\n",
    "    if per_param:\n",
    "        r2_scores = []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            ss_res = torch.sum((y_true[:, i] - y_pred[:, i])**2)\n",
    "            ss_tot = torch.sum((y_true[:, i] - torch.mean(y_true[:, i]))**2)\n",
    "            if ss_tot == 0: r2 = 1.0 if ss_res < 1e-9 else 0.0\n",
    "            else: r2 = 1 - ss_res / ss_tot\n",
    "            r2_scores.append(r2.item())\n",
    "        return r2_scores\n",
    "    else:\n",
    "        ss_res = torch.sum((y_true - y_pred)**2)\n",
    "        ss_tot = torch.sum((y_true - torch.mean(y_true))**2)\n",
    "        if ss_tot == 0: return 1.0 if ss_res < 1e-9 else 0.0\n",
    "        return (1 - ss_res / ss_tot).item()\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, checkpoint_path=\"best_model.pth\"):\n",
    "        self.patience = patience; self.min_delta = min_delta; self.checkpoint_path = checkpoint_path\n",
    "        self.counter = 0; self.best_loss = float('inf'); self.early_stop = False\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss; self.counter = 0\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience: print(\"Early stopping triggered.\"); self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "# --- 5. Training and Evaluation Loop (MODIFIED for LR Warmup) ---\n",
    "def train_model(model, train_loader, optimizer, criterion, device, param_means, param_stds, epoch, train_config):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # LR Warmup\n",
    "    base_lr = train_config[\"learning_rate\"]\n",
    "    warmup_epochs = train_config[\"warmup_epochs\"]\n",
    "    if epoch <= warmup_epochs:\n",
    "        # Linear warmup: lr = base_lr * (current_epoch / warmup_epochs)\n",
    "        # Ensure epoch is 1-indexed for this formula\n",
    "        lr = base_lr * (epoch / warmup_epochs)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    elif epoch == warmup_epochs + 1 : # After warmup, set to base_lr (if no other scheduler)\n",
    "         for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = base_lr\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch} (LR: {current_lr:.2e})\")\n",
    "    for batch_idx, (data, target_params_orig) in enumerate(pbar):\n",
    "        data = data.to(device)\n",
    "        target_params_orig = target_params_orig.to(device)\n",
    "        target_params_norm = (target_params_orig - param_means.to(device)) / param_stds.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions_norm = model(data)\n",
    "        loss = criterion(predictions_norm, target_params_norm)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 50 == 0:\n",
    "             pbar.set_postfix({\"Loss\": loss.item()})\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss, current_lr\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device, param_names_list, param_means, param_stds, is_test_set=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions_denorm, all_targets_denorm = [], []\n",
    "    desc = \"Testing\" if is_test_set else \"Evaluating\"\n",
    "    with torch.no_grad():\n",
    "        for data, target_params_orig in tqdm(data_loader, desc=desc, leave=False):\n",
    "            data, target_params_orig = data.to(device), target_params_orig.to(device)\n",
    "            target_params_norm = (target_params_orig - param_means.to(device)) / param_stds.to(device)\n",
    "            predictions_norm = model(data)\n",
    "            loss = criterion(predictions_norm, target_params_norm)\n",
    "            total_loss += loss.item()\n",
    "            predictions_denorm = denormalize_params(predictions_norm, param_means.to(device), param_stds.to(device))\n",
    "            all_predictions_denorm.append(predictions_denorm.cpu())\n",
    "            all_targets_denorm.append(target_params_orig.cpu())\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    all_predictions_denorm = torch.cat(all_predictions_denorm, dim=0)\n",
    "    all_targets_denorm = torch.cat(all_targets_denorm, dim=0)\n",
    "    r2_scores_per_param = r_squared(all_targets_denorm, all_predictions_denorm, per_param=True)\n",
    "    print_prefix = \"Test\" if is_test_set else \"Validation\"\n",
    "    print(f\"\\n--- {print_prefix} Results ---\")\n",
    "    print(f\"{print_prefix} Loss (norm): {avg_loss:.6f}\")\n",
    "    for i, name in enumerate(param_names_list): print(f\"{print_prefix} R² for {name} (denorm): {r2_scores_per_param[i]:.4f}\")\n",
    "    print(\"-------------------------\\n\")\n",
    "    return avg_loss, r2_scores_per_param\n",
    "\n",
    "# --- 6. Parameter Sweep Function (UNCHANGED) ---\n",
    "def perform_parameter_sweep(model, fixed_params_base, param_to_sweep_idx, sweep_values,\n",
    "                            T, n_series_per_value, device,\n",
    "                            return_norm_mean, return_norm_std, \n",
    "                            param_norm_means, param_norm_stds, \n",
    "                            param_names_list):\n",
    "    model.eval()\n",
    "    results_pred = {name: [] for name in param_names_list}\n",
    "    true_swept_values_recorded = []\n",
    "    print(f\"\\n--- Performing Parameter Sweep for {param_names_list[param_to_sweep_idx]} ---\")\n",
    "    for sweep_val in tqdm(sweep_values, desc=f\"Sweeping {param_names_list[param_to_sweep_idx]}\", leave=False):\n",
    "        current_params_true_denorm = np.copy(fixed_params_base)\n",
    "        current_params_true_denorm[param_to_sweep_idx] = sweep_val\n",
    "        true_swept_values_recorded.append(sweep_val)\n",
    "        batch_returns_list_for_sweep = []\n",
    "        for _ in range(n_series_per_value):\n",
    "            series_single = simulate_svj_series_vectorized(current_params_true_denorm.reshape(1, -1), T, param_names_list)\n",
    "            batch_returns_list_for_sweep.append(torch.tensor(series_single, dtype=torch.float32))\n",
    "        batch_returns_for_sweep = torch.cat(batch_returns_list_for_sweep, dim=0)\n",
    "        batch_returns_norm, _, _ = normalize_returns(batch_returns_for_sweep, return_norm_mean, return_norm_std)\n",
    "        batch_returns_norm = batch_returns_norm.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions_norm = model(batch_returns_norm)\n",
    "            predictions_denorm = denormalize_params(predictions_norm, param_norm_means.to(device), param_norm_stds.to(device))\n",
    "            avg_predictions_denorm = predictions_denorm.mean(dim=0).cpu().numpy()\n",
    "        for i, name in enumerate(param_names_list): results_pred[name].append(avg_predictions_denorm[i])\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 7)); param_name_swept = param_names_list[param_to_sweep_idx]\n",
    "    plt.plot(true_swept_values_recorded, results_pred[param_name_swept], 'o-', label=f'Avg. Predicted {param_name_swept}')\n",
    "    plt.plot(true_swept_values_recorded, true_swept_values_recorded, 'k--', label='True Value (Ideal)')\n",
    "    plt.xlabel(f'True {param_name_swept}'); plt.ylabel(f'Predicted {param_name_swept}')\n",
    "    plt.title(f'Parameter Sweep Analysis: {param_name_swept}'); plt.legend(); plt.grid(True)\n",
    "    sweep_plot_filename = f\"sweep_{param_name_swept}.png\"; plt.savefig(sweep_plot_filename)\n",
    "    print(f\"Parameter sweep plot saved to {sweep_plot_filename}\"); plt.close()\n",
    "    return results_pred, true_swept_values_recorded\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    X_train_raw, Y_train_raw = generate_dataset_vectorized(CONFIG[\"N_TRAIN\"], CONFIG[\"T\"], CONFIG[\"PARAM_NAMES\"])\n",
    "    X_val_raw, Y_val_raw = generate_dataset_vectorized(CONFIG[\"N_VAL\"], CONFIG[\"T\"], CONFIG[\"PARAM_NAMES\"])\n",
    "    X_test_raw, Y_test_raw = generate_dataset_vectorized(CONFIG[\"N_TEST\"], CONFIG[\"T\"], CONFIG[\"PARAM_NAMES\"])\n",
    "\n",
    "    X_train_norm, return_mean_norm, return_std_norm = normalize_returns(X_train_raw)\n",
    "    X_val_norm, _, _ = normalize_returns(X_val_raw, return_mean_norm, return_std_norm)\n",
    "    X_test_norm, _, _ = normalize_returns(X_test_raw, return_mean_norm, return_std_norm)\n",
    "    _, param_means_norm, param_stds_norm = normalize_params(Y_train_raw)\n",
    "\n",
    "    train_dataset = SVJDataset(X_train_norm, Y_train_raw)\n",
    "    val_dataset = SVJDataset(X_val_norm, Y_val_raw)\n",
    "    test_dataset = SVJDataset(X_test_norm, Y_test_raw)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"TRAINING_PARAMS\"][\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"TRAINING_PARAMS\"][\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"TRAINING_PARAMS\"][\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    model = SVJTransformer(\n",
    "        num_params_to_predict=len(CONFIG[\"PARAM_NAMES\"]),\n",
    "        T_input=CONFIG[\"T\"],\n",
    "        model_config=CONFIG[\"MODEL_PARAMS\"]\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"TRAINING_PARAMS\"][\"learning_rate\"], weight_decay=1e-4) # AdamW is often better\n",
    "    criterion = nn.MSELoss()\n",
    "    early_stopper = EarlyStopping(patience=CONFIG[\"TRAINING_PARAMS\"][\"early_stopping_patience\"], checkpoint_path=CONFIG[\"MODEL_SAVE_PATH\"])\n",
    "    writer = SummaryWriter(CONFIG[\"TENSORBOARD_LOG_DIR\"])\n",
    "    print(f\"Model outputting to {CONFIG['TENSORBOARD_LOG_DIR']} and {CONFIG['MODEL_SAVE_PATH']}\")\n",
    "    print(f\"Model initialized. CNN output seq len: {model.cnn_output_seq_len}, Positional encoding max_len: {model.pos_encoder.pe.size(1)}\")\n",
    "    print(f\"Total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    for epoch in range(1, CONFIG[\"TRAINING_PARAMS\"][\"epochs\"] + 1):\n",
    "        start_time = time.time()\n",
    "        train_loss, current_lr = train_model(model, train_loader, optimizer, criterion, device, param_means_norm, param_stds_norm, epoch, CONFIG[\"TRAINING_PARAMS\"])\n",
    "        val_loss, val_r2_scores = evaluate_model(model, val_loader, criterion, device, CONFIG[\"PARAM_NAMES\"], param_means_norm, param_stds_norm)\n",
    "        end_time = time.time()\n",
    "        print(f\"Epoch {epoch}/{CONFIG['TRAINING_PARAMS']['epochs']} - Time: {end_time-start_time:.2f}s - LR: {current_lr:.2e} - Train Loss (norm): {train_loss:.6f} - Val Loss (norm): {val_loss:.6f}\")\n",
    "        writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
    "        for i, name in enumerate(CONFIG[\"PARAM_NAMES\"]): writer.add_scalar(f'R2_Validation/{name}', val_r2_scores[i], epoch)\n",
    "        if early_stopper(val_loss, model): break\n",
    "    \n",
    "    writer.close()\n",
    "    print(\"--- Training Finished ---\")\n",
    "\n",
    "    print(\"\\n--- Evaluating on Test Set with Best Model ---\")\n",
    "    if os.path.exists(CONFIG[\"MODEL_SAVE_PATH\"]):\n",
    "        model.load_state_dict(torch.load(CONFIG[\"MODEL_SAVE_PATH\"]))\n",
    "        test_loss, test_r2_scores = evaluate_model(model, test_loader, criterion, device, CONFIG[\"PARAM_NAMES\"], param_means_norm, param_stds_norm, is_test_set=True)\n",
    "    else:\n",
    "        print(f\"Warning: Model checkpoint {CONFIG['MODEL_SAVE_PATH']} not found. Skipping test set evaluation with best model.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Starting Parameter Sweep Example ---\")\n",
    "    if os.path.exists(CONFIG[\"MODEL_SAVE_PATH\"]):\n",
    "        fixed_params_base_for_sweep = Y_train_raw.mean(dim=0).numpy()\n",
    "        param_to_sweep = 'beta'; param_to_sweep_idx = CONFIG[\"PARAM_NAMES\"].index(param_to_sweep)\n",
    "        beta_sim_range = (0.79, 0.99); beta_sweep_values = np.linspace(beta_sim_range[0], beta_sim_range[1], 10)\n",
    "        perform_parameter_sweep(model, fixed_params_base_for_sweep, param_to_sweep_idx, beta_sweep_values,\n",
    "                                CONFIG[\"T\"], 20, device, return_mean_norm, return_std_norm,\n",
    "                                param_means_norm, param_stds_norm, CONFIG[\"PARAM_NAMES\"])\n",
    "    else:\n",
    "        print(f\"Warning: Model checkpoint {CONFIG['MODEL_SAVE_PATH']} not found. Skipping parameter sweep.\")\n",
    "    print(\"--- Script Finished ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
