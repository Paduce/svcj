{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import dataclasses\n",
    "import math\n",
    "import time\n",
    "from typing import Callable, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from svcj.config import DATA_DIR, IS_START, IS_END, MONGO_URI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# 1 ▸ Parameter container & realistic hyper-cube (crypto-calibrated)\n",
    "# ===================================================================================\n",
    "@dataclasses.dataclass\n",
    "class SVCJParams:\n",
    "    \"\"\"All structural parameters of the Stochastic Volatility with Correlated Jumps (SVCJ) model.\"\"\"\n",
    "\n",
    "    mu: float\n",
    "    kappa: float\n",
    "    theta: float\n",
    "    sigma_v: float\n",
    "    rho: float\n",
    "    lam: float  # jump intensity (λ)\n",
    "    mu_s: float  # jump in log-returns\n",
    "    sigma_s: float\n",
    "    mu_v: float  # jump in variance\n",
    "    sigma_vj: float\n",
    "    rho_j: float\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "    # Priors\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "    @staticmethod\n",
    "    def sample_crypto_prior(\n",
    "        rng: np.random.Generator | None = None,\n",
    "    ) -> \"SVCJParams\":\n",
    "        \"\"\"Draw one parameter set from a hand-crafted crypto prior hyper-cube.\"\"\"\n",
    "        rng = rng or np.random.default_rng()\n",
    "        return SVCJParams(\n",
    "            mu=rng.uniform(-0.0005, 0.0005),\n",
    "            kappa=rng.uniform(0.01, 6.0),\n",
    "            theta=rng.uniform(1e-6, 1e-2),\n",
    "            sigma_v=rng.uniform(1e-4, 0.1),\n",
    "            rho=rng.uniform(-0.4,0.4),\n",
    "            lam=rng.uniform(0.0, 10.0),\n",
    "            mu_s=rng.uniform(-0.1, 0.1),\n",
    "            sigma_s=rng.uniform(0.01, 0.2),\n",
    "            mu_v=rng.uniform(1e-6, 1e-2),\n",
    "            sigma_vj=rng.uniform(1e-6, 1e-2),\n",
    "            rho_j=rng.uniform(-0.4, 0.4),\n",
    "        )\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "    # Convenience helpers\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "    def as_tensor(self, *, device: torch.device | str | None = None) -> torch.Tensor:\n",
    "        \"\"\"(22,) float32 tensor view of the parameters.\"\"\"\n",
    "        return torch.tensor(dataclasses.astuple(self), dtype=torch.float32, device=device)\n",
    "\n",
    "    @staticmethod\n",
    "    def fields() -> List[str]:\n",
    "        return list(SVCJParams.__annotations__.keys())\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "# 2 ▸ SVCJ path simulator (Euler–Maruyama + Poisson jumps)\n",
    "# ===================================================================================\n",
    "class SVCJSimulator:\n",
    "    \"\"\"Generates one path of log-returns under the SVCJ dynamics.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def simulate(\n",
    "        params: SVCJParams,\n",
    "        steps: int,\n",
    "        *,\n",
    "        dt: float = 1 / 24,  # default: hourly for 24/7 markets\n",
    "        rng: np.random.Generator | None = None,\n",
    "    ) -> np.ndarray:\n",
    "        rng = rng or np.random.default_rng()\n",
    "\n",
    "        (\n",
    "            mu,\n",
    "            kappa,\n",
    "            theta,\n",
    "            sigma_v,\n",
    "            rho,\n",
    "            lam,\n",
    "            mu_s,\n",
    "            sig_s,\n",
    "            mu_v,\n",
    "            sig_vj,\n",
    "            _,\n",
    "        ) = dataclasses.astuple(params)\n",
    "\n",
    "        # Brownian shocks (correlated)\n",
    "        dW_s = rng.normal(0.0, math.sqrt(dt), size=steps)\n",
    "        dW_v = rng.normal(0.0, math.sqrt(dt), size=steps)\n",
    "        dW_v = rho * dW_s + math.sqrt(max(1e-8, 1.0 - rho * rho)) * dW_v\n",
    "\n",
    "        # Poisson jump counts & magnitudes\n",
    "        dN = rng.poisson(lam * dt, size=steps)\n",
    "        Z_s = rng.normal(mu_s, sig_s, size=steps) * dN\n",
    "        Z_v = rng.normal(mu_v, sig_vj, size=steps) * dN\n",
    "\n",
    "        # Path containers\n",
    "        V: float = theta  # start at long-run level\n",
    "        rets = np.empty(steps, dtype=np.float32)\n",
    "\n",
    "        # Euler–Maruyama recursion\n",
    "        for t in range(steps):\n",
    "            V_pos = max(V, 0.0)\n",
    "            rets[t] = (\n",
    "                mu * dt\n",
    "                - 0.5 * V_pos * dt\n",
    "                + math.sqrt(V_pos) * dW_s[t]\n",
    "                + Z_s[t]\n",
    "            )\n",
    "            V = max(\n",
    "                V + kappa * (theta - V) * dt + sigma_v * math.sqrt(V_pos) * dW_v[t] + Z_v[t],\n",
    "                1e-12,\n",
    "            )\n",
    "        return rets\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "# 3 ▸ PyTorch Dataset that streams *fresh* simulations each epoch\n",
    "# ===================================================================================\n",
    "class SVCJDataset(Dataset):\n",
    "    \"\"\"An *in-finite* dataset: every __getitem__ produces brand-new synthetic data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int,\n",
    "        *,\n",
    "        min_T: int = 240,\n",
    "        max_T: int = 720,\n",
    "        rng: np.random.Generator | None = None,\n",
    "    ) -> None:\n",
    "        self.n = n_samples\n",
    "        self.min_T = min_T\n",
    "        self.max_T = max_T\n",
    "        self.rng = rng or np.random.default_rng()\n",
    "\n",
    "    # Dataset protocol -------------------------------------------------------\n",
    "    def __len__(self) -> int:  # type: ignore[override]\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:  # type: ignore[override]\n",
    "        \"\"\"Return one *variable-length* return series + ground-truth params.\"\"\"\n",
    "        T = int(self.rng.integers(self.min_T, self.max_T + 1))\n",
    "        psi = SVCJParams.sample_crypto_prior(self.rng)\n",
    "        r = SVCJSimulator.simulate(psi, T, rng=self.rng)\n",
    "        x = torch.from_numpy(r).float().unsqueeze(0)  # (1, T)\n",
    "        y = psi.as_tensor()\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Collate & worker helpers\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def svcj_collate(batch: Sequence[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "    \"\"\"Right-pad variable-length sequences so they can be stacked.\"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    lengths = [x.size(-1) for x in xs]\n",
    "    max_len = max(lengths)\n",
    "    padded = torch.zeros(len(xs), 1, max_len, dtype=xs[0].dtype)\n",
    "    for i, x in enumerate(xs):\n",
    "        padded[i, :, -x.size(-1) :] = x  # right-align keeps *most recent* info at the end\n",
    "    return padded.contiguous(), torch.stack(ys)\n",
    "\n",
    "\n",
    "def svcj_worker_init(worker_id: int):\n",
    "    \"\"\"Guarantee *independent* RNG streams across data-loader workers.\"\"\"\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    dataset: SVCJDataset = worker_info.dataset  # type: ignore[arg-type]\n",
    "    seed = dataset.rng.integers(0, 2**32 - 1) + worker_id\n",
    "    dataset.rng = np.random.default_rng(int(seed))\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "# 4 ▸ Temporal Convolutional Network (variable-length input)\n",
    "# ===================================================================================\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp: int):\n",
    "        super().__init__()\n",
    "        self.chomp = chomp\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        return x[..., : -self.chomp] if self.chomp else x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_c: int, out_c: int, *, k: int = 3, d: int = 1, p: float = 0.1):\n",
    "        super().__init__()\n",
    "        pad = (k - 1) * d\n",
    "        self.net = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Conv1d(in_c, out_c, k, padding=pad, dilation=d)),\n",
    "            Chomp1d(pad),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.utils.weight_norm(nn.Conv1d(out_c, out_c, k, padding=pad, dilation=d)),\n",
    "            Chomp1d(pad),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.down = nn.Conv1d(in_c, out_c, 1) if in_c != out_c else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        return torch.relu(self.net(x) + self.down(x))\n",
    "\n",
    "\n",
    "class SVCJTCN(nn.Module):\n",
    "    \"\"\"Back-bone for amortised parameter inference.\"\"\"\n",
    "\n",
    "    def __init__(self, *, n_layers: int = 6, n_filters: int = 64, k: int = 3):\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        in_c = 1\n",
    "        for i in range(n_layers):\n",
    "            layers.append(ResidualBlock(in_c, n_filters, k=k, d=2**i))\n",
    "            in_c = n_filters\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_filters, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 22),\n",
    "        )\n",
    "        self.register_buffer(\"var_floor\", torch.tensor(1e-6))\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────────\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:  # type: ignore[override]\n",
    "        h = self.tcn(x)\n",
    "        stats = self.head(h)\n",
    "        mean, log_var = stats.split(11, dim=-1)\n",
    "        var = F.softplus(log_var) + self.var_floor  # strictly >0, better gradients\n",
    "        return mean, var\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "# 5 ▸ UKF Refinement via Unscented Kalman Filter for parameter smoothing\n",
    "# ===================================================================================\n",
    "class UKFRefiner:\n",
    "    \"\"\"Optional temporal smoothing of per-window estimates via a vanilla UKF.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        n_steps: int = 3,\n",
    "        alpha: float = 1e-3,\n",
    "        beta: float = 2.0,\n",
    "        kappa: float = 0.0,\n",
    "    ) -> None:\n",
    "        self.n_steps = n_steps\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def _unscented_transform(\n",
    "        self, f: Callable[[torch.Tensor], torch.Tensor], m: torch.Tensor, P: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        n = m.size(-1)\n",
    "        lambda_ = self.alpha**2 * (n + self.kappa) - n\n",
    "        c = n + lambda_\n",
    "        Wm0 = lambda_ / c\n",
    "        Wc0 = Wm0 + (1 - self.alpha**2 + self.beta)\n",
    "        Wi = 0.5 / c\n",
    "        sqrt_P = torch.linalg.cholesky(P * c)\n",
    "        sigma = torch.cat([m.unsqueeze(0), m.unsqueeze(0) + sqrt_P, m.unsqueeze(0) - sqrt_P], dim=0)\n",
    "        Y = f(sigma)  # shape: (2n+1, n)\n",
    "        y_mean = Wm0 * Y[0] + Wi * Y[1:].sum(0)\n",
    "        Yc = Y - y_mean\n",
    "        Pyy = Wc0 * torch.outer(Yc[0], Yc[0]) + Wi * (Yc[1:].T @ Yc[1:])\n",
    "        return y_mean, Pyy\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def __call__(\n",
    "        self,\n",
    "        mean: torch.Tensor,\n",
    "        var: torch.Tensor,\n",
    "        x: torch.Tensor,\n",
    "        m_prior: torch.Tensor | None = None,\n",
    "        S_prior: torch.Tensor | None = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if m_prior is None or S_prior is None:\n",
    "            return mean, var\n",
    "        m = m_prior.clone()\n",
    "        P = S_prior.clone()\n",
    "        R = torch.diag(var)\n",
    "\n",
    "        # Identity measurement function — amortiser already outputs state vector\n",
    "        identity: Callable[[torch.Tensor], torch.Tensor] = lambda z: z\n",
    "\n",
    "        for _ in range(self.n_steps):\n",
    "            y_mean, P_zz = self._unscented_transform(identity, m, P)\n",
    "            P_zz = P_zz + R\n",
    "            K = P @ torch.linalg.inv(P_zz)\n",
    "            m = m + K @ (mean - y_mean)\n",
    "            P = P - K @ P_zz @ K.mT  # keep symmetric\n",
    "        return m, torch.diagonal(P)\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "# 6 ▸ Trainer wrapper\n",
    "# ===================================================================================\n",
    "class SVCJTrainer:\n",
    "    def __init__(self, model: nn.Module, *, device: str = \"cuda\") -> None:\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.opt = torch.optim.AdamW(self.model.parameters(), lr=2e-4, weight_decay=1e-2)\n",
    "        self.register_buffer = lambda *args, **kwargs: None  # placeholder so that .to() works\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def train_epoch(self, loader: DataLoader, *, beta: float = 3.0) -> None:\n",
    "        self.model.train()\n",
    "        tot_loss = 0.0\n",
    "        for x, y in loader:\n",
    "            x = x.to(self.device, non_blocking=True)\n",
    "            y = y.to(self.device, non_blocking=True)\n",
    "\n",
    "            mean, var = self.model(x)\n",
    "            inv_var = var.reciprocal()\n",
    "            log_det = torch.log(var + 1e-9)\n",
    "\n",
    "            nll = 0.5 * ((y - mean).pow(2) * inv_var + log_det).sum(-1).mean()\n",
    "            penalty = beta * F.relu(((y - mean).abs() - 2.0 * torch.sqrt(var)).sum(-1)).mean()\n",
    "            loss = nll + penalty\n",
    "\n",
    "            self.opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.opt.step()\n",
    "            tot_loss += loss.item()\n",
    "        return tot_loss / len(loader)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def fit(\n",
    "        self,\n",
    "        *,\n",
    "        n_epochs: int = 5,\n",
    "        steps_per_epoch: int = 500,\n",
    "        batch_size: int = 512,\n",
    "        num_workers: int = 4,\n",
    "    ) -> None:\n",
    "        dataset = SVCJDataset(steps_per_epoch * batch_size)\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=svcj_collate,\n",
    "            worker_init_fn=svcj_worker_init,\n",
    "        )\n",
    "        for ep in range(1, n_epochs + 1):\n",
    "            t0 = time.time()\n",
    "            loss = self.train_epoch(loader)\n",
    "            print(f\"[epoch {ep:02d}/{n_epochs}] loss={loss:8.5f}  ({time.time() - t0:4.1f}s)\")\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "# 7 ▸ Inference utility — rolling parameter time-series\n",
    "# ===================================================================================\n",
    "class SVCJInference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        *,\n",
    "        refiner: UKFRefiner | None = None,\n",
    "        device: str = \"cuda\",\n",
    "    ) -> None:\n",
    "        self.model = model.eval().to(device)\n",
    "        self.refiner = refiner or UKFRefiner()\n",
    "        self.device = device\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        returns: np.ndarray | torch.Tensor,\n",
    "        *,\n",
    "        refine: bool = False,\n",
    "        m_prev: torch.Tensor | None = None,\n",
    "        S_prev: torch.Tensor | None = None,\n",
    "        lambda_f: float = 2.0,\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Predict model output (mean, var) for a given (batched or unbatched) array of returns.\n",
    "        Arguments:\n",
    "            returns: shape (..., window_length)\n",
    "        Returns:\n",
    "            mean, var as numpy arrays (squeezed to 1d for a single sample, or 2d for batch)\n",
    "        \"\"\"\n",
    "        # Convert input to tensor\n",
    "        if isinstance(returns, np.ndarray):\n",
    "            x = torch.from_numpy(returns).float().to(self.device)\n",
    "        else:\n",
    "            x = returns.float().to(self.device)\n",
    "        # Reshape to [batch, 1, window]\n",
    "        if x.ndim == 1:\n",
    "            x = x.unsqueeze(0).unsqueeze(0)\n",
    "        elif x.ndim == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            mean, var = self.model(x)\n",
    "        mean, var = mean.squeeze(0), var.squeeze(0)\n",
    "        # Refinement if requested\n",
    "        if refine:\n",
    "            mean, var = self.refiner(mean, var, x.squeeze(0), m_prev, S_prev)\n",
    "        return mean.cpu().numpy(), var.cpu().numpy()\n",
    "\n",
    "    def rolling_estimates(\n",
    "        self,\n",
    "        prices: np.ndarray,\n",
    "        timestamps: np.ndarray | None = None,\n",
    "        *,\n",
    "        window: int = 720,\n",
    "        refine: bool = False,\n",
    "        lambda_f: float = 2.0,\n",
    "    ) -> \"pd.DataFrame\":\n",
    "        import pandas as pd\n",
    "\n",
    "        if timestamps is None:\n",
    "            timestamps = np.arange(len(prices))\n",
    "\n",
    "        logp = np.log(prices.astype(np.float64))\n",
    "        returns = np.diff(logp).astype(np.float32)\n",
    "\n",
    "        means: list[np.ndarray] = []\n",
    "        vars_: list[np.ndarray] = []\n",
    "        m_prev: torch.Tensor | None = None\n",
    "        S_prev: torch.Tensor | None = None\n",
    "\n",
    "        for t in range(window, len(returns) + 1):\n",
    "            window_returns = returns[t - window : t]\n",
    "            mean, var = self.predict(\n",
    "                window_returns,\n",
    "                refine=refine,\n",
    "                m_prev=m_prev,\n",
    "                S_prev=S_prev,\n",
    "                lambda_f=lambda_f,\n",
    "            )\n",
    "            means.append(mean)\n",
    "            vars_.append(var)\n",
    "            # Update state for refiner if needed\n",
    "            m_prev = torch.from_numpy(mean).to(self.device)\n",
    "            S_prev = torch.diag(torch.from_numpy(var * lambda_f).to(self.device))\n",
    "\n",
    "        # Timestamps for result index\n",
    "        index = (\n",
    "            pd.to_datetime(timestamps[window:], unit=\"s\")\n",
    "            if np.issubdtype(timestamps.dtype, np.integer)\n",
    "            else pd.to_datetime(timestamps[window:])\n",
    "        )\n",
    "        cols = SVCJParams.fields()\n",
    "        df_mean = pd.DataFrame(means, index=index, columns=cols)\n",
    "        df_var = pd.DataFrame(vars_, index=index, columns=[f\"{c}_var\" for c in cols])\n",
    "        return pd.concat([df_mean, df_var], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2, 1, 624]) y: torch.Size([2, 11])\n",
      "Forward pass ok: torch.Size([2, 11]) torch.Size([2, 11])\n"
     ]
    }
   ],
   "source": [
    "ds = SVCJDataset(n_samples=10)\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    batch_size=2,\n",
    "    collate_fn=svcj_collate,\n",
    "    worker_init_fn=svcj_worker_init,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Check one batch\n",
    "for x, y in loader:\n",
    "    print(\"x:\", x.shape, \"y:\", y.shape)\n",
    "    break\n",
    "\n",
    "# Instantiate model & trainer\n",
    "model   = SVCJTCN(n_layers=4, n_filters=32)\n",
    "trainer = SVCJTrainer(model, device=\"cpu\")\n",
    "print(\"Forward pass ok:\", model(x)[0].shape, model(x)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pduce/sbt/svcj/.venv/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/tmp/ipykernel_45788/820212936.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: set()\n",
      "Predicted params (mean): [-0.0805743   0.04756197  2.744004   -0.10452211  0.05089341 -0.04276178\n",
      " -0.15153016 -0.06141418 -0.0877592   0.054356   -0.06689388]\n",
      "True params: [-3.5826949e-04  5.1593947e+00  1.4571112e-03  8.5954919e-02\n",
      " -1.2422550e-01  3.9173887e+00  3.8847264e-02  1.6806702e-01\n",
      "  9.1629885e-03  7.2722770e-03 -7.0708744e-02]\n",
      "          predicted      true\n",
      "mu        -0.080574 -0.000358\n",
      "kappa      0.047562  5.159395\n",
      "theta      2.744004  0.001457\n",
      "sigma_v   -0.104522  0.085955\n",
      "rho        0.050893 -0.124225\n",
      "lam       -0.042762  3.917389\n",
      "mu_s      -0.151530  0.038847\n",
      "sigma_s   -0.061414  0.168067\n",
      "mu_v      -0.087759  0.009163\n",
      "sigma_vj   0.054356  0.007272\n",
      "rho_j     -0.066894 -0.070709\n"
     ]
    }
   ],
   "source": [
    "# Load model from checkpoint\n",
    "# Instantiate model first\n",
    "model = SVCJTCN(n_layers=6, n_filters=64, k=3)  # Reduce n_layers to 6 to match checkpoint\n",
    "\n",
    "# Then load checkpoint\n",
    "ckpt_path = \"svcj_tcn.pt\"\n",
    "state_dict = torch.load(ckpt_path)\n",
    "\n",
    "# Print missing keys before loading\n",
    "missing_keys = set(model.state_dict().keys()) - set(state_dict.keys())\n",
    "print(\"Missing keys:\", missing_keys)\n",
    "\n",
    "# Load state dict with strict=False to ignore missing keys\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "test_len = 2  # Can be > or < T_TRAIN\n",
    "test_dataset = SVCJDataset(n_samples=1, min_T=test_len, max_T=test_len)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=svcj_collate,\n",
    "    worker_init_fn=svcj_worker_init,\n",
    "    num_workers=1,\n",
    ")\n",
    "x, y = list(test_loader)[0]\n",
    "\n",
    "# Or: infer on any raw series directly!\n",
    "infer = SVCJInference(model=model, device='cuda')\n",
    "mean, var = infer.predict(returns=x[0])  # or returns=x.squeeze(0)\n",
    "print(\"Predicted params (mean):\", mean)\n",
    "print(\"True params:\", y[0].numpy())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\n",
    "    pd.concat(\n",
    "        [pd.Series(mean, index=SVCJParams.fields()), pd.Series(y[0].cpu().numpy(), index=SVCJParams.fields())],\n",
    "        axis=1, keys=[\"predicted\", \"true\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-277.757141</td>\n",
       "      <td>-3818.169922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-16.126276</td>\n",
       "      <td>3.095801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23931.474609</td>\n",
       "      <td>16067.860352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-18.298306</td>\n",
       "      <td>0.005242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.967661</td>\n",
       "      <td>-0.097543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-34.681225</td>\n",
       "      <td>3.239611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-36.031876</td>\n",
       "      <td>0.037132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-120.537987</td>\n",
       "      <td>0.033082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>119.918839</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27.665014</td>\n",
       "      <td>0.004818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-13.013965</td>\n",
       "      <td>0.315070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1\n",
       "0    -277.757141  -3818.169922\n",
       "1     -16.126276      3.095801\n",
       "2   23931.474609  16067.860352\n",
       "3     -18.298306      0.005242\n",
       "4      10.967661     -0.097543\n",
       "5     -34.681225      3.239611\n",
       "6     -36.031876      0.037132\n",
       "7    -120.537987      0.033082\n",
       "8     119.918839      0.000057\n",
       "9      27.665014      0.004818\n",
       "10    -13.013965      0.315070"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3818.1699)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
